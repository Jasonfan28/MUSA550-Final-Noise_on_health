[
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Transportation Noise and its Impact on Health Outcomes in the Northeast",
    "section": "",
    "text": "MUSA 5500 Geospatial Data Science in Python | Jason Fan"
  },
  {
    "objectID": "home.html#project-overview",
    "href": "home.html#project-overview",
    "title": "Transportation Noise and its Impact on Health Outcomes in the Northeast",
    "section": "Project Overview",
    "text": "Project Overview\nDoes the roar of a highway or the rumble of a train affect your physical and mental health? This project investigates the relationship between transportation noise pollution and various health outcomes across counties in the Northeast United States.\nBy integrating National Transportation Noise Map data with CDC PLACES health data and US Census socioeconomic indicators, we aim to uncover statistical links between our acoustic environment and our well-being.\nRead the Analysis →"
  },
  {
    "objectID": "home.html#key-findings-at-a-glance",
    "href": "home.html#key-findings-at-a-glance",
    "title": "Transportation Noise and its Impact on Health Outcomes in the Northeast",
    "section": "Key Findings at a Glance",
    "text": "Key Findings at a Glance\n\n\nThe “Expectation of Quiet”\nPsychological sensitivity to noise is linked to privilege. High-income/education groups report the highest depression rates (~24%) despite living in the quietest areas, suggesting a lower tolerance for environmental disturbance.\n\n\nThe “Cosmopolitan Buffer”\nA striking paradox exists in dense urban cores: residents report the lowest depression (~15%) yet suffer the highest cholesterol (~36%). The social vibrancy of the city protects the mind, but the noise silently taxes the body.\n\n\nThe Hidden “Body Tax”\nWhile lower-income groups report less psychological distress (likely due to habituation), their bodies keep the score. We observed a classic Environmental Justice pattern where noise exposure correlates strongly with high cholesterol (Allostatic Load) in lower-education populations."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "Findings: The Divergence Between Perceived and Physiological Stress\n\nThe “Expectation of Quiet” vs. Biological Reality\nThe most critical insight from this study is that environmental quality does not impact mental and physical health in lockstep. Instead, we found a sharp disconnect between how different populations feel about noise and how their bodies physically react to it.\nPsychologically, the data supports an “Expectation of Quiet.” It is actually the wealthiest and most educated residents who show the steepest rise in depression as noise levels increase. This suggests that for privileged groups, noise is a violation of their environmental standards, leading to acute annoyance and reported distress. Conversely, lower-income groups appear to habituate to the noise, reporting lower depression levels likely because environmental sound is deprioritized amidst more immediate survival stressors.\nHowever, the physiological markers tell a different, more concerning story. Following a classic Environmental Justice pattern, lower-education populations show a strong correlation between noise exposure and high cholesterol. Even if they do not report feeling “stressed” or depressed, their bodies are absorbing the wear and tear of the environment—a phenomenon known as allostatic load.\n\n\nThe “Cosmopolitan Buffer” and Hidden Costs\nThis paradox becomes even clearer when we look at specific neighborhood archetypes. The most striking contradiction is found in Cluster 2 (“The Cosmopolitan Core”)—dense, diverse, and noisy urban centers populated by high-income residents.\nResidents here benefit from a “Cosmopolitan Buffer.” They report the lowest rates of depression (~15%), suggesting that they psychologically offset the annoyance of noise with the social and economic vibrancy of city life. But this resilience comes with a hidden receipt: this same group suffers from the highest prevalence of high cholesterol (~36%). While the amenities of the city protect their mood, the high-stimulation environment is exacting a silent physiological tax.\n\n\nThe Burden of the Periphery\nOn the other end of the spectrum is Cluster 3 (“The Quiet Elite”). These residents enjoy the lowest noise levels and best physical health (lowest cholesterol), yet they report the highest rates of depression (~24%). This reinforces the idea that sensitivity to noise is a byproduct of privilege; without the distraction of urban life, and perhaps isolated by suburban design, these residents are hyper-aware of environmental disturbances.\nUltimately, the heaviest cost falls on Cluster 0 (“The Vulnerable Periphery”). Comprising lower-income, older populations in high-noise areas, this group lacks both the “amenity buffer” of the city center and the “protective wealth” of the elite suburbs. Consequently, they experience moderately high levels of both depression and physiological stress, confirming that the most vulnerable populations lack the resources to decouple their health from their environment.\n\n\n\nnoise map"
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data Analysis Process",
    "section": "",
    "text": "We start by importing the necessary libraries for geospatial analysis and machine learning."
  },
  {
    "objectID": "data-analysis.html#environment-setup-configuration",
    "href": "data-analysis.html#environment-setup-configuration",
    "title": "Data Analysis Process",
    "section": "",
    "text": "We start by importing the necessary libraries for geospatial analysis and machine learning."
  },
  {
    "objectID": "data-analysis.html#data-gathering",
    "href": "data-analysis.html#data-gathering",
    "title": "Data Analysis Process",
    "section": "2. Data Gathering",
    "text": "2. Data Gathering\n\nProcessing Noise Raster Data\nThe National Transportation Noise Map is provided as individual high-resolution TIFF files for each state. To analyze the Northeast as a contiguous region, I needed to merge these into a single mosaic.\nBecause the uncompressed raster data exceeds 50GB, merging them into a physical file is inefficient and computationally expensive. Instead, I utilized GDAL to build a Virtual Raster (VRT). A VRT acts as a pointer file that treats the collection of state rasters as a single seamless map without duplicating the underlying data.\n\n\nData Ingestion Pipeline\nTo streamline the process, I created a DataPipeline class. This class handles the connection to the CDC API, downloads county boundaries, and manages the noise raster file paths.\n\n\nUtility Functions\nWe also defined helper functions for multicollinearity checks (VIF) and correlation plotting.\n\n\nLoading the Data\nNote: Since API calls and spatial joins are computationally expensive, the code below demonstrates the logic used. For this website report, we load the pre-processed GeoJSON files directly.\n\n\nFetching Census Socioeconomic Data\nWe target the 2023 American Community Survey (ACS) 5-Year Estimates for this analysis. The 5-year estimates are preferred over 1-year data for this project because they provide higher statistical precision for areas with smaller populations, smoothing out annual volatility in rural counties. We specifically query variables that serve as robust proxies for Social Determinants of Health (SDOH):\n\nEconomic Stability: Median_Income and Poverty_Count are variables which measure community wealth and economic deprivation. Low-income communities often face higher exposure to environmental stressors and have fewer resources to mitigate them (e.g., soundproofing, healthcare access).\nEnvironmental Justice: White_Count, Asian_Count,Hispanic_Count, and Black_Count are included to test for potential disparities in noise exposure. Environmental justice literature suggests that transportation infrastructure is disproportionately sited near communities of color. Senior_Count also looks at if older adults are more impacted by noise and health outcomes.\nEducation: Bachelor_Degree_Count is included as educational attainment is a strong predictor of health literacy and overall socioeconomic status, often providing explanatory power beyond income alone."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project Description",
    "section": "",
    "text": "Noise pollution is often an overlooked environmental stressor. While air pollution’s effects are well-documented, the chronic stress caused by transportation noise, from road, rail, and aviation, may have significant impacts on cardiovascular and mental health."
  },
  {
    "objectID": "project-description.html#motivation",
    "href": "project-description.html#motivation",
    "title": "Project Description",
    "section": "",
    "text": "Noise pollution is often an overlooked environmental stressor. While air pollution’s effects are well-documented, the chronic stress caused by transportation noise, from road, rail, and aviation, may have significant impacts on cardiovascular and mental health."
  },
  {
    "objectID": "project-description.html#data-sources",
    "href": "project-description.html#data-sources",
    "title": "Project Description",
    "section": "Data Sources",
    "text": "Data Sources\nThree primary datasets were utilized for this analysis:\n\nNational Transportation Noise Map (2020):\n\nSource: US Department of Transportation.\nFormat: Raster data (.tif/vrt).\nMetric: 24-hour equivalent sound level (LAeq24).\n\nCDC PLACES Data (2023):\n\nSource: Centers for Disease Control and Prevention.\nFormat: API / GeoJSON.\nMetrics: County-level prevalence of 18 health outcomes (e.g., Depression, Stroke, Asthma).\n\nUS Census ACS (2021):\n\nSource: American Community Survey (5-Year Estimates).\nMetrics: Poverty rates, median income, racial demographics, and education levels (used as control variables)."
  },
  {
    "objectID": "project-description.html#methodology",
    "href": "project-description.html#methodology",
    "title": "Project Description",
    "section": "Methodology",
    "text": "Methodology\nThe analysis focused on the Northeast United States (PA, NJ, NY, CT, RI, MA, NH, VT, ME). We performed geospatial processing to aggregate noise raster data to the county level using population-weighted zonal statistics. We then employed Ridge Regression and Random Forest models to isolate the effect of noise on health outcomes while controlling for socioeconomic confounders."
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Overview of Methodology",
    "section": "",
    "text": "The analysis focused on the Northeast United States (PA, NJ, NY, CT, RI, MA, NH, VT, ME)."
  },
  {
    "objectID": "methods.html#analytical-approach",
    "href": "methods.html#analytical-approach",
    "title": "Overview of Methodology",
    "section": "Analytical Approach",
    "text": "Analytical Approach\n\nGeospatial Processing: We aggregated high-resolution noise raster data to the county level using population-weighted zonal statistics.\nStatistical Modeling: We employed Ridge Regression and Random Forest models to isolate the effect of noise on health outcomes while controlling for socioeconomic confounders.\n\nFor the detailed code and step-by-step implementation, please see the Data Analysis section."
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "Ridge Regression\n\n\n\nTable 1: Robust Ridge Regression Results: Noise Impact on Health\n\n\n\n\n\nHealth Outcome\nR-Squared\nNoise Beta\n\n\n\n\nDepression\n0.696\n-0.009\n\n\nHigh blood pressure\n0.637\n-0.089\n\n\nStroke\n0.804\n0.004\n\n\nArthritis\n0.717\n-0.097\n\n\nCancer (non-skin) or melanoma\n0.955\n-0.012\n\n\nChronic obstructive pulmonary disease\n0.753\n0.024\n\n\nCognitive disability\n0.430\n0.029\n\n\nCoronary heart disease\n0.750\n-0.007\n\n\nCurrent asthma\n0.512\n0.049\n\n\nDiagnosed diabetes\n0.670\n-0.011\n\n\nFair or poor self-rated health status\n0.673\n0.032\n\n\nFrequent mental distress\n0.706\n-0.010\n\n\nHearing disability\n0.735\n0.012\n\n\nHigh cholesterol\n0.091\n-0.119\n\n\nMobility disability\n0.738\n0.045\n\n\nObesity\n0.501\n-0.210\n\n\nVision disability\n0.873\n0.027\n\n\n\n\n\n\n\n\n\nInital VIF\n\n\nWOW that VIF is bad! Therefore I got rid of features until it wasn’t bad\n\n\n\nmid vif\n\n\nWhile Log_income has a VIF above 5, I’ll be getting rid of something that might be correlated to the income such as poverty. This is due to income showing a lot more about a place such as being wealthy or not, while poverty only shows one statistic.\n\n\n\nFinal VIF\n\n\nNow all VIF is below 5 showing no multicollinearity, meaning that each variables are completely independent of each other.\n\n\nP Value Test\nFound very little significance, with p-values all being high, and a large amount of multicollinearity. Therefore, I started to look for potential interaction terms. The assumption for these results is due to socioeconomic variables having too much correlation.\n\n\n\ninteraction model\n\n\n\nInital Assumption:\nNoise pollution is an equity issue. It disproportionately impacts mental health in lower-income communities, while wealthier residents are buffered from its effects.\nWealthy/High-Degree Areas: The noise doesn’t bother them as much. Maybe they have better soundproofing, better windows, or the noise is “positive” noise (bustling cafes) rather than “negative” noise (highways/industrial).\nLower-Degree Areas: The relationship between Noise and Depression is stronger. Here, noise acts as a stressor that compounds with financial stress.\n\n\n\nLooking for other interactions using this model\nCreated a new predictor that is ‘avg_noise_db’ * ‘Pct_Degree’ to represent more than just the people with no degree like before.\n \nDepression along with High Cholesterol appeared to have significant interactions.\n\n\nPlotting\n\n\n\nInteraction between noise and depression\n\n\n\nImpact of noise on depression accounting for education level:\n\nRelative Deprivation: Higher socioeconomic standard individuals may feel more “entitled” to quiet environments. When that expectation is violated, the psychological toll (annoyance, frustration, depression) is higher.\nAwareness vs. Survival: Lower socioeconomic standard populations often face a “hierarchy of needs” where immediate stressors (financial instability, food security, crime) outweigh environmental noise. They may “tune out” the noise psychologically because it is just one of many stressors, resulting in lower reported noise-related depression, even if the noise is high.\n\n\n\n\nInteraction between noise and cholesterol\n\n\n\n\nImpact of noise on cholesterol accounting for education level:\n\nThe Body Keeps the Score: Even if lower-education populations do not report higher depression, their bodies are biologically reacting to the noise stressor. The noise is triggering cortisol production and raising cholesterol, regardless of their conscious “acceptance” of the noise.\nProtective Factors for High SES: The highly educated group might be annoyed by the noise, but they have the resources to mitigate the physical damage (better diet, healthcare access, better sound-insulation in homes), keeping their cholesterol levels decoupled from the noise levels.\n\n\n\n\nCluster Analysis\nConducted cluster analysis to see the spatial split of how these factors impact specific communities.\n\n\n\nK cluster\n\n\nWhile the drop is great between cluster 1 and 2, it greatly decreases between 2 and 3 and 4. Too vague to fully tell which one to use\n\n\n\nSilhouette score\n\n\nWhile k=2 is the highest silhouette score, having only 2 clusters isn’t going to be helpful.\nTherefore, I will be using k=4 as the second highest score\n\n\n\ncluster\n\n\nCluster 0: Highest noise, lowest income/education. oldest pop, and white (low income, old, white pop with high noise)\nCluster 1: average across the board (middle ground)\nCluster 2: Highest income, high education, relatively high noise, very diverse, and pop density (major urban center with wealthy educated and diverse people, still high noise)\nCluster 3: Lowest noice, highest education (Wealthy highly educated suburbs with little noise stress)\n\nBoxplot\n \nCluster 2: low depression and high cholestrol. (wealthy, diverse, and living in high density and high noise environments. They don’t report feeling depressed but they have the highest cholesterol showing worst physiological stress indicators.)\nCluster 3: low cholestrol but high depression (has the quiet but bad mental health. Could be that they are accustomed to quiet environments, that they have a lower threshold for noise causing them distress.)\nCluster 0: low income, older, and high noise exposer. They get hit with everythihng\n\n\n\nmap\n\n\nBased on the box plots and spatial analysis, these findings reveal a striking “Paradox of Place” where psychological well-being and physiological health are inversely related across neighborhood types.\nCluster 2 (The Cosmopolitan Core) presents the most compelling contradiction: residents here report the lowest rates of depression (~15%) yet suffer from the highest rates of high cholesterol (~36%). This suggests a “Cosmopolitan Buffer,” where the economic opportunities, social vibrancy, and amenities of dense urban living protect mental health, causing residents to psychologically “discount” the environmental stressors. However, their bodies still keep the score—the high noise and density create a hidden physiological tax (allostatic load) that manifests as high cholesterol, even if they don’t feel “stressed.”\nConversely, Cluster 3 (The Quiet Elite) shows the exact opposite pattern: they have the lowest cholesterol (indicating good physical health and protective resources) but the highest reported depression (~24%). This supports the “Expectation of Quiet” hypothesis; residents in these privileged, lower-noise areas may be more sensitive to environmental disturbances or suffer from suburban isolation, leading to higher reported mental distress despite their physical safety. Meanwhile, Cluster 0 (The Vulnerable Periphery) lacks the protective buffers of either group, showing moderately high levels of both depression and cholesterol, reflecting the compounding burdens of lower socioeconomic status and environmental exposure."
  },
  {
    "objectID": "data-analysis.html#zonal-statistics-to-merge-the-data",
    "href": "data-analysis.html#zonal-statistics-to-merge-the-data",
    "title": "Data Analysis Process",
    "section": "Zonal statistics to merge the data",
    "text": "Zonal statistics to merge the data\nNote: I calculated arithmetic mean for decibel value which is physically incorrect due to dB being logarithmic. By using arithmetic mean, this significantly underestimates total noise exposure. While it would be more accurate to convert the raster to acoustic energy, do the zonal stats on energy, then convert back to dB, my abilities and laptop are far too limited to do this. Making this a limitation of my analysis.\n\nStandardization & Merge\nBefore merging, we must ensure our primary keys match perfectly. FIPS (Federal Information Processing Standards) codes are often read as integers by default, which strips the leading zero from states like Alabama, California, or Connecticut."
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! I’m Jason Tianchi Fan, a Master of City Planning student at the University of Pennsylvania. \n\n\n\n\n\nThat’s me :)"
  },
  {
    "objectID": "stats.html#wow-that-vif-is-bad",
    "href": "stats.html#wow-that-vif-is-bad",
    "title": "Regression Analysis",
    "section": "WOW that VIF is bad!",
    "text": "WOW that VIF is bad!\ntherefore i got rid of features until it wasn’t bad\n\n\n\nTable 3: Variance Inflation Factors (VIF) - Adjusted Model\n\n\n\n\n\nFeature\nVIF\n\n\n\n\nLog_Income\n3.73\n\n\nPct_Degree\n3.26\n\n\nTotal_Pop\n2.71\n\n\nPct_Black\n2.61\n\n\nPct_Asian\n2.54\n\n\nPct_Hispanic\n2.28\n\n\nPct_Senior\n2.06\n\n\nLog_Noise_Exposure\n1.07\n\n\n\n\n\n\nNow all VIF is below 5 showing no multicollinearity, meaning that each variables are completely independent of each other.\n\nP Value Test\n\n\n\nTable 4: Regression Results: Noise Exposure vs. Health Outcomes\n\n\n\n\n\nHealth Outcome\nImpact (Coef)\nP-Value\nR-Squared\nObs\n\n\n\n\nObesity\n-12.818\n0.180\n0.686\n218\n\n\nCancer (non-skin) or melanoma\n-1.110\n0.201\n0.943\n218\n\n\nCoronary heart disease\n-1.796\n0.276\n0.805\n218\n\n\nArthritis\n-7.142\n0.327\n0.753\n218\n\n\nHigh blood pressure\n-5.230\n0.407\n0.680\n218\n\n\nStroke\n-0.733\n0.415\n0.769\n218\n\n\nCurrent asthma\n2.066\n0.486\n0.428\n218\n\n\nDiagnosed diabetes\n-1.857\n0.501\n0.715\n218\n\n\nDepression\n2.735\n0.696\n0.695\n218\n\n\nChronic obstructive pulmonary disease\n-1.080\n0.708\n0.779\n218\n\n\nFair or poor self-rated health status\n-2.439\n0.718\n0.697\n218\n\n\nHearing disability\n-0.466\n0.833\n0.774\n218\n\n\nHigh cholesterol\n-1.410\n0.839\n0.212\n218\n\n\nVision disability\n0.344\n0.842\n0.781\n218\n\n\nMobility disability\n-0.514\n0.912\n0.730\n218\n\n\nFrequent mental distress\n-0.331\n0.920\n0.705\n218\n\n\nCognitive disability\n0.235\n0.965\n0.591\n218\n\n\n\n\n\n\nNoise pollution is an equity issue. It disproportionately impacts mental health in lower-income communities, while wealthier residents are buffered from its effects.\nWealthy/High-Degree Areas: The noise doesn’t bother them as much. Maybe they have better soundproofing, better windows, or the noise is “positive” noise (bustling cafes) rather than “negative” noise (highways/industrial).\nLower-Degree Areas: The relationship between Noise and Depression is stronger. Here, noise acts as a stressor that compounds with financial stress.\n\nThe Red Line (Low Education): It is effectively flat and high. Meaning, in lower-education neighborhoods, depression rates are consistently high (~24%) regardless of whether it is quiet or loud.\nThe Blue Line (High Education): It is steep and upward sloping.Meaning in quiet areas (left side), high-education populations have very low depression (~18%). But as noise increases (right side), their depression rates skyrocket, eventually catching up to the low-education group.\n\n\n\nCluster Analysis\n While the drop is great between cluster 1 and 2, it greatly decreases between 2 and 3 and 4. Too vague to fully tell which one to use\nWhile k=2 is the highest silhouette score, having only 2 clusters isn’t going to be helpful.\nTherefore, I will be using k=4 as the second highest score\n | Cluster | Archetype | Description | |:—:|:—|:—| | 0 | Struggling / Rural | Small, rural/industrial areas facing economic challenges. | | 1 | Diverse Working-Class | Large, diverse cities with a strong working-class demographic. | | 2 | Stable Mid-Sized | Mid-sized cities that are traditional and economically stable. | | 3 | Wealthy Mega-Cities | Huge, educated population centers with high income levels. |\n: Cluster Profiles Summary {#tbl-clusters}\n\n\n\ncluster box\n\n\n\n\n\nmap\n\n\nSpatial correlations * cluster 0 (Rust Belt) High Depression (24.5%): These areas correlate with the “Deaths of Despair” literature—economic stagnation often links to poorer mental health outcomes.\nLow Education (89% No Degree): The lack of higher education institutions or the \"brain drain\" of young people leaving these rural counties likely drives this.\n\nSeniors: With a 21.6% senior population, these are aging counties.\n\nNoise Paradox: It is fascinating that this cluster had the highest noise (54.38 dBA) in your table. In a rural county context, this might be capturing the fact that populations are clustered tightly around highways/truck routes in valleys, or it might suggest that \"quiet\" wilderness doesn't offset the noise where people actually live.\n\nCluster 2 (New England) High Seniors (21.8%): This is your oldest cluster. Northern New England (VT/NH/ME) has some of the oldest median ages in the US.\nHigh Depression: Like Cluster 0, this group struggles with mental health. The combination of aging populations and potentially seasonal isolation (long winters in VT/NH/ME) could be a factor here distinct from the economic distress of Cluster 0.\nCluster 1 & 3 (I 95) The “Protected” Class (Cluster 3): This group (Pop 1M+, High Income) likely represents the wealthy suburban counties (e.g., Westchester, Montgomery, Fairfax). Their wealth insulates them from the depression prevalence seen in the rural areas.\nThe Diverse Hubs (Cluster 1): With high Hispanic (19%) and Black (13%) populations, these are likely the diverse, urbanized counties that are economically active but have mixed outcomes."
  },
  {
    "objectID": "code_testing.html",
    "href": "code_testing.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\n\nCode\n# --- IMPORTS ---\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport rasterio\nimport subprocess\nfrom rasterstats import zonal_stats\nfrom sodapy import Socrata\nfrom shapely.geometry import shape\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport warnings\n\n# --- CONFIGURATION ---\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")\n\n\n\n\nCode\n#getting the noise data\ndata_dir = \"data/CONUS_rail_road_and_aviation_noise_2020/State_rasters/\"\nsearch_pattern = os.path.join(data_dir, \"*_rail_road_and_aviation_noise_2020.tif\")\noutput_vrt = os.path.join(data_dir, \"CONUS_merged_noise_map_2020.vrt\")\n\ntif_files = glob.glob(search_pattern)\n\nif not tif_files:\n    print(f\"--- ERROR ---\")\n    print(f\"No .tif files were found at: {search_pattern}\")\nelse:\n    print(f\"Found {len(tif_files)} state raster files to link.\")\n    \n    command_list = [\"gdalbuildvrt\", output_vrt] + tif_files\n    \n    print(\"Building VRT file...\")\n    \n    try:\n        subprocess.run(command_list, check=True, shell=True)\n        \n        print(f\"\\n--- Success! ---\")\n        print(f\"Virtual Raster .vrt file created at:\")\n        print(output_vrt)\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"\\n--- GDAL ERROR ---\")\n        print(f\"The command failed: {e}\")\n        print(\"Please ensure GDAL is installed in your active Conda environment.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n\nFound 49 state raster files to link.\nBuilding VRT file...\n\n--- Success! ---\nVirtual Raster .vrt file created at:\ndata/CONUS_rail_road_and_aviation_noise_2020/State_rasters/CONUS_merged_noise_map_2020.vrt\n\n\n\n\nCode\ndata_dir = \"data/CONUS_rail_road_and_aviation_noise_2020/State_rasters/\"\nNOISE_RASTER_PATH = os.path.join(data_dir, \"CONUS_merged_noise_map_2020.vrt\")\nCDC_APP_TOKEN = \"ReEZWmd3hqYSQ879UCZyhtP1T\" \n\n\n\n\nCode\n# --- HELPER CLASS ---\nclass DataPipeline:\n    \"\"\"\n    Handles data ingestion and geometry processing for CDC and Census data.\n    \"\"\"\n    def __init__(self, cdc_token, noise_path):\n        self.cdc_token = cdc_token\n        self.noise_path = noise_path\n        self.NE_STATES = ['PA', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'VT', 'ME']\n\n    def get_cdc_data(self, limit=50000):\n        \"\"\"Fetches and cleans CDC PLACES data.\"\"\"\n        print(f\"Fetching {limit} records from CDC API...\")\n        client = Socrata(\"data.cdc.gov\", self.cdc_token)\n        dataset_id = \"swc5-untb\"\n        results = client.get(dataset_id, limit=limit)\n        \n        gdf = gpd.GeoDataFrame.from_records(results)\n        \n        # Geometry processing\n        gdf = gdf.dropna(subset=['geolocation'])\n        gdf['geometry'] = gdf['geolocation'].apply(lambda x: shape(x))\n        gdf = gdf.set_geometry('geometry', crs=\"EPSG:4326\")\n        \n        # Filter for NE States\n        gdf = gdf[gdf['stateabbr'].isin(self.NE_STATES)]\n        \n        # Numeric conversion\n        gdf['data_value'] = pd.to_numeric(gdf['data_value'], errors='coerce')\n        return gdf\n\n    def get_counties(self):\n        \"\"\"Fetches US County boundaries.\"\"\"\n        print(\"Downloading County Boundaries...\")\n        url = \"https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\"\n        counties = gpd.read_file(url)\n        counties = counties.to_crs(\"EPSG:4326\")\n        \n        # Create FIPS\n        counties['FIPS'] = counties['STATEFP'] + counties['COUNTYFP']\n        return counties[['FIPS', 'geometry']]\n\n# --- UTILITY FUNCTIONS ---\ndef calculate_vif(df):\n    \"\"\"Calculates Variance Inflation Factor to check for multicollinearity.\"\"\"\n    # Handle infinite/nan values just in case\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n    return vif_data.sort_values('VIF', ascending=False)\n\ndef plot_correlation_matrix(df, title=\"Correlation Matrix\"):\n    \"\"\"Plots a publication-quality heatmap.\"\"\"\n    plt.figure(figsize=(10, 8))\n    corr = df.corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    sns.heatmap(corr, mask=mask, cmap='RdBu_r', center=0, square=True, linewidths=.5)\n    plt.title(title, fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n# Initialize Pipeline\npipeline = DataPipeline(CDC_APP_TOKEN, NOISE_RASTER_PATH)\n\n\n\n\nCode\n# 1. Load Data\ncdc_gdf = pipeline.get_cdc_data(limit=250000)\ncounties_gdf = pipeline.get_counties()\n\n# 2. Pivot CDC Data (Wide Format)\n# We need rows = Counties, Columns = Health Outcomes + SES\nprint(\"Pivoting CDC data...\")\nhealth_pivot = cdc_gdf.pivot_table(\n    index=['locationid', 'locationname', 'stateabbr'],\n    columns='measure',\n    values='data_value',\n    aggfunc='mean'\n).reset_index()\n\n\nFetching 250000 records from CDC API...\nDownloading County Boundaries...\nPivoting CDC data...\n\n\n\n\nCode\n# 3. Merge with Geometries\nmodel_gdf = counties_gdf.merge(health_pivot, left_on='FIPS', right_on='locationid', how='inner')\n\n\n\n\nCode\nmodel_gdf = model_gdf.set_geometry('geometry')\n\nprint(f\"Final Dataset: {len(model_gdf)} counties ready for analysis.\")\n\n\nFinal Dataset: 218 counties ready for analysis.\n\n\n\n\nCode\nPA = counties_gdf[counties_gdf['FIPS'].str.startswith('42')]\n\n\n\n\nCode\nimport rasterio\nfrom rasterio.mask import mask\nfrom rasterio.plot import show\nfrom rasterio.windows import from_bounds\nfrom rasterio.enums import Resampling\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import mapping\nimport numpy as np\n\nDOWNSAMPLE_FACTOR = 0.1\n\nwith rasterio.open(NOISE_RASTER_PATH) as src:\n        model_gdf = model_gdf.to_crs(src.crs)\n        minx, miny, maxx, maxy = model_gdf.total_bounds\n        window = from_bounds(minx, miny, maxx, maxy, src.transform)\n        new_height = int(window.height * DOWNSAMPLE_FACTOR)\n        new_width = int(window.width * DOWNSAMPLE_FACTOR)\n        out_image = src.read(\n        1,\n        window=window,\n        out_shape=(new_height, new_width),\n        resampling=Resampling.bilinear\n    )\n\nnoise_data_masked = np.ma.masked_less_equal(out_image, 0)\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\nimg = ax.imshow(\n    noise_data_masked, \n    cmap='inferno_r', \n    extent=(minx, maxx, miny, maxy),\n    origin='upper'\n)\n\n# Formatting\nplt.colorbar(img, ax=ax, label=\"Noise Level (dB)\", fraction=0.01, pad=0.04)\nplt.title(\"Transportation Noise: North East\", fontsize=16, fontweight='bold')\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nax.set_axis_off()\n\nplt.tight_layout\nplt.show()\n\n\n\n\n\n\n\n\n\nFirst attempted regression\n\n\nCode\n# 1. Calculate Naive Zonal Statistics (Arithmetic Mean)\n# Note: This is physically incorrect for decibels, but demonstrated for baseline comparison.\nwith rasterio.open(NOISE_RASTER_PATH) as src:\n    target_crs = src.crs\n    nodata_val = src.nodata\n\nmodel_gdf = model_gdf.to_crs(target_crs)\n\nstats_arithmetic = zonal_stats(\n    model_gdf,\n    NOISE_RASTER_PATH,\n    stats=['mean'],\n    all_touched=True,\n    band=1,\n    nodata=nodata_val\n)\n\nresult_gdf = pd.DataFrame(stats_arithmetic)\nresult_gdf['locationid'] = model_gdf['locationid'].values\nresult_gdf = result_gdf.rename(columns={'mean': 'avg_noise_db'})\nresult_gdf = result_gdf.dropna(subset=['avg_noise_db'])\n\n\n\n\nCode\nmodel_gdf = model_gdf.merge(\n    result_gdf,\n    on='locationid',\n    how='left'\n)\n\n\n\n\nCode\n# TARGET (y) = Noise \n# FEATURES (X) = All Health Outcomes\ntarget = 'avg_noise_db'\nfeatures = [col for col in model_gdf.columns if col not in ['FIPS', 'geometry', 'locationid', 'locationname', 'stateabbr', 'naive_noise_db']]\n\nX = model_gdf[features].dropna()\ny = model_gdf.loc[X.index, target]\n\n\n\n\nCode\n# Run Random Forest\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nrf_naive = RandomForestRegressor(random_state=42)\nrf_naive.fit(X_train, y_train)\n\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n1.0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\n# Diagnostic Outputs\nprint(f\"\\n Model R² Score: {rf_naive.score(X_test, y_test):.4f}\")\n\nvif_df = calculate_vif(X)\nprint(vif_df.head(10))\n\n# Plot Correlation\nplot_correlation_matrix(X)\n\n\n\n Model R² Score: 0.9620\n                                              feature          VIF\n9                 Coronary heart disease among adults  8052.084383\n38  Visited dentist or dental clinic in the past y...  7944.017177\n32                  Self-care disability among adults  7598.641118\n35  Taking medicine to control high blood pressure...  7556.890581\n19            Frequent physical distress among adults  7468.313397\n39  Visits to doctor for routine checkup within th...  6741.279607\n37                     Vision disability among adults  6636.673746\n34                                Stroke among adults  6292.508869\n15  Fair or poor self-rated health status among ad...  6247.298957\n18              Frequent mental distress among adults  5350.840512\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 1. DEFINE CENSUS FETCHING FUNCTION\n# ---------------------------------------------------------\ndef fetch_census_data_standalone():\n    \"\"\"\n    Fetches 2023 ACS 5-Year Data via US Census API for Northeast States.\n    Returns a cleaned DataFrame ready for merging.\n    \"\"\"\n    print(\"Fetching US Census ACS Data (Income, Poverty, Race, Education)...\")\n    \n    # NE State FIPS: CT, ME, MA, NH, NJ, NY, PA, RI, VT\n    ne_fips = [\"09\", \"23\", \"25\", \"33\", \"34\", \"36\", \"42\", \"44\", \"50\"]\n    \n    # Map API variables to readable names\n    vars_map = {\n        \"B01003_001E\": \"Total_Pop\",\n        \"B19013_001E\": \"Median_Income\",\n        \"B17001_002E\": \"Poverty_Count\",\n        \"B03002_003E\": \"White_Count\",\n        \"B03002_004E\": \"Black_Count\",\n        \"B15003_022E\": \"Bachelor_Degree_Count\"\n    }\n    \n    var_string = \",\".join(vars_map.keys())\n    dfs = []\n    \n  \n    for fips in ne_fips:\n        url = f\"https://api.census.gov/data/2023/acs/acs5?get=NAME,{var_string}&for=county:*&in=state:{fips}\"\n        try:\n            df = pd.read_json(url)\n            df.columns = df.iloc[0]\n            df = df[1:]\n            dfs.append(df)\n        except Exception as e:\n                print(f\"   ! Error fetching state {fips}: {e}\")\n        \n    if not dfs:\n         raise ValueError(\"Failed to fetch Census data.\")\n    \n    census = pd.concat(dfs, ignore_index=True)\n    \n    # Create 5-digit FIPS for merging (State + County)\n    census['FIPS'] = census['state'] + census['county']\n    \n    # Rename columns\n    census = census.rename(columns=vars_map)\n    \n    # Convert to Numeric\n    for col in vars_map.values():\n        census[col] = pd.to_numeric(census[col], errors='coerce')\n        \n    census['Pct_Poverty'] = (census['Poverty_Count'] / census['Total_Pop']) * 100\n    census['Pct_White'] = (census['White_Count'] / census['Total_Pop']) * 100\n    census['Pct_Black'] = (census['Black_Count'] / census['Total_Pop']) * 100\n    census['Pct_Degree'] = (census['Bachelor_Degree_Count'] / census['Total_Pop']) * 100\n    census['Log_Income'] = np.log1p(census['Median_Income'])\n    \n    # Select final columns\n    return census[['FIPS', 'Total_Pop', 'Log_Income', 'Pct_Poverty', 'Pct_White', 'Pct_Black', 'Pct_Degree']]\n\n\n\n\nCode\ncensus_df = fetch_census_data_standalone()\n\n\nFetching US Census ACS Data (Income, Poverty, Race, Education)...\n\n\n\n\nCode\n# Ensure key types match (strings)\nmodel_gdf['FIPS'] = model_gdf['locationid'].astype(str).str.zfill(5)\ncensus_df['FIPS'] = census_df['FIPS'].astype(str).str.zfill(5)\n\n# Perform Merge\nfinal_df = model_gdf.merge(census_df, left_on='FIPS', right_on='FIPS', how='left')\n\n# Physics Correction: Log-Transform the Noise (Decibels are logarithmic, but health impacts often scale linearly with Log(Energy))\n# Note: We use the 'avg_noise_db' you calculated in your previous cells\nfinal_df['Log_Noise_Exposure'] = np.log1p(final_df['avg_noise_db'])\n\nprint(f\"Final Dataset with Census Data: {len(final_df)} counties.\")\n\n\nFinal Dataset with Census Data: 218 counties.\n\n\nCT decided to change their county shape in addition to the total number of county’s they have from 8 to 9. This change happened on August 2024, which means there is no way to get ACS 5 data from CT as the earliest ACS 5 data we can get is in 2023.\nThe two options I have is either to use the less reliable ACS 1 year data’s, or I\n\n\nCode\n# 3. RUN CORRECTED REGRESSION LOOP\n# ---------------------------------------------------------\nprint(\"\\n--- RUNNING ROBUST RIDGE REGRESSIONS ---\")\n\n# Define inputs (X) - Socioeconomics + Noise\nfeatures = ['Log_Noise_Exposure', 'Log_Income', 'Pct_Poverty', 'Pct_Black', 'Pct_Degree']\n\n# Define Health Outcomes (y)\noutcomes_of_interest = [\n    'Depression among adults',\n    'High blood pressure among adults', \n    'Stroke among adults',\n    'Sleep duration &lt; 7 hours among adults aged &gt;=18 years', \n    'Arthritis among adults', \n    'Cancer (non-skin) or melanoma among adults', \n    'Chronic obstructive pulmonary disease among adults', \n    'Cognitive disability among adults', \n    'Coronary heart disease among adults', \n    'Current asthma among adults', \n    'Diagnosed diabetes among adults', \n    'Fair or poor self-rated health status among adults', \n    'Frequent mental distress among adults',\n    'Hearing disability among adults',\n    'High cholesterol among adults who have ever been screened', \n    'Mobility disability among adults', \n    'Obesity among adults', \n    'Vision disability among adults'\n]\n\nresults_list = []\n\nfor outcome in outcomes_of_interest:\n    # Skip if outcome not in data\n    if outcome not in final_df.columns:\n        continue\n        \n    # Drop NaNs for this specific combination\n    df_mod = final_df.dropna(subset=features + [outcome])\n    \n    if len(df_mod) &lt; 10:\n        print(f\"Skipping {outcome}: Not enough data.\")\n        continue\n\n    X = df_mod[features]\n    y = df_mod[outcome]\n    \n    # Split Data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    # Scale Data (Important for comparing coefficients of Noise vs Income)\n    scaler = StandardScaler()\n    X_train_sc = scaler.fit_transform(X_train)\n    X_test_sc = scaler.transform(X_test)\n    \n    # Ridge Regression (Handles multicollinearity between Income/Poverty better than LinearRegression)\n    model = Ridge(alpha=1.0)\n    model.fit(X_train_sc, y_train)\n    \n    # Calculate R2\n    r2 = r2_score(y_test, model.predict(X_test_sc))\n    \n    # Extract Noise Coefficient (Index 0 is 'Log_Noise_Exposure')\n    noise_coef = model.coef_[0]\n    \n    results_list.append({\n        'Health Outcome': outcome.split(\" among\")[0],\n        'R2 Score': r2,\n        'Noise Impact (Beta)': noise_coef\n    })\n    \n    print(f\"Outcome: {outcome[:15]}... | R²: {r2:.3f} | Noise Beta: {noise_coef:.3f}\")\n\n\n\n--- RUNNING ROBUST RIDGE REGRESSIONS ---\nOutcome: Depression amon... | R²: 0.605 | Noise Beta: 0.037\nOutcome: High blood pres... | R²: 0.607 | Noise Beta: -0.120\nOutcome: Stroke among ad... | R²: 0.731 | Noise Beta: -0.002\nOutcome: Arthritis among... | R²: 0.639 | Noise Beta: -0.101\nOutcome: Cancer (non-ski... | R²: 0.714 | Noise Beta: -0.007\nOutcome: Chronic obstruc... | R²: 0.738 | Noise Beta: 0.017\nOutcome: Cognitive disab... | R²: 0.603 | Noise Beta: -0.009\nOutcome: Coronary heart ... | R²: 0.649 | Noise Beta: -0.017\nOutcome: Current asthma ... | R²: 0.333 | Noise Beta: 0.031\nOutcome: Diagnosed diabe... | R²: 0.578 | Noise Beta: -0.028\nOutcome: Fair or poor se... | R²: 0.638 | Noise Beta: -0.035\nOutcome: Frequent mental... | R²: 0.697 | Noise Beta: -0.015\nOutcome: Hearing disabil... | R²: 0.737 | Noise Beta: -0.001\nOutcome: High cholestero... | R²: -0.023 | Noise Beta: -0.128\nOutcome: Mobility disabi... | R²: 0.691 | Noise Beta: 0.013\nOutcome: Obesity among a... | R²: 0.595 | Noise Beta: -0.274\nOutcome: Vision disabili... | R²: 0.736 | Noise Beta: -0.007\n\n\n\n\nCode\nvif_features = ['Log_Noise_Exposure', 'Log_Income', 'Pct_Poverty', 'Pct_Black', 'Pct_Degree']\nX_vif = final_df[vif_features].dropna()\nX_vif['const'] = 1\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = X_vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) \n                   for i in range(len(X_vif.columns))]\nvif_display = vif_data[vif_data[\"Feature\"] != 'const'].sort_values('VIF', ascending=False)\nprint(vif_display)\n\n\n              Feature       VIF\n1          Log_Income  4.571799\n2         Pct_Poverty  3.019654\n4          Pct_Degree  2.863944\n3           Pct_Black  1.609137\n0  Log_Noise_Exposure  1.058018\n\n\nAll features have a VIF below 5, showing low correlation between features\n\n\nCode\ncorr_matrix = final_df[vif_features].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    corr_matrix, \n    annot=True,       # Show the numbers in the boxes\n    fmt=\".2f\",        # Round to 2 decimal places\n    cmap='RdBu_r',    # Red/Blue diverging colormap (Red=Pos, Blue=Neg)\n    center=0,         # Center the colormap at 0\n    vmin=-1, vmax=1,  # Fix scale from -1 to 1\n    square=True,      # Force square aspect ratio\n    linewidths=.5,    # Add lines between squares\n    cbar_kws={\"shrink\": .8}\n)\nplt.title('Correlation Matrix of Regression Features', fontsize=16)\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 4. VISUALIZE RESULTS\n# ---------------------------------------------------------\nif results_list:\n    res_df = pd.DataFrame(results_list)\n\n    plt.figure(figsize=(12, 5))\n\n    # Plot 1: R2 Scores\n    plt.subplot(1, 2, 1)\n    sns.barplot(data=res_df, x='R2 Score', y='Health Outcome', palette='viridis')\n    plt.title(\"Model Fit (R²)\")\n    plt.xlabel(\"Variance Explained (0-1)\")\n    plt.xlim(0, 1)\n\n    # Plot 2: Noise Coefficients\n    plt.subplot(1, 2, 2)\n    sns.barplot(data=res_df, x='Noise Impact (Beta)', y='Health Outcome', palette='magma')\n    plt.title(\"Standardized Impact of Noise\")\n    plt.xlabel(\"Beta Coefficient (Positive = Harmful)\")\n    plt.axvline(0, color='black', linestyle='--')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No matching health outcomes found to plot.\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ==========================================\n# PHASE 3: SPATIAL VISUALIZATION\n# ==========================================\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef plot_noise_map(gdf, noise_col='avg_noise_db', title='Average Transportation Noise by County'):\n    \"\"\"\n    Plots a choropleth map of noise levels.\n    \"\"\"\n    # 1. Prepare Data (Ensure it's a GeoDataFrame)\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        print(\"Converting to GeoDataFrame...\")\n        gdf = gpd.GeoDataFrame(gdf, geometry='geometry')\n    \n    # Check if column exists\n    if noise_col not in gdf.columns:\n        # Fallback to other likely names from previous steps\n        options = ['Noise_Mean_dB', 'noise_db', 'Log_Noise_Exposure']\n        for opt in options:\n            if opt in gdf.columns:\n                noise_col = opt\n                print(f\"Using '{noise_col}' for plotting.\")\n                break\n        else:\n            print(f\"Error: Could not find noise column '{noise_col}' in dataset.\")\n            return\n\n    # 2. Setup Plot\n    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\n    # 3. Plot\n    gdf.plot(\n        column=noise_col,\n        ax=ax,\n        legend=True,\n        cax=cax,\n        cmap='inferno_r', \n        legend_kwds={'label': \"Noise Level (dB)\"},\n        missing_kwds={'color': 'lightgrey', 'label': 'Missing Data'}\n    )\n\n    # 4. Formatting\n    ax.set_title(title, fontsize=16, fontweight='bold')\n    ax.set_axis_off() \n    \n    plt.tight_layout()\n    plt.show()\n\n# --- EXECUTE PLOT ---\n# Try to plot using the 'final_df' from Phase 2, or fallback to 'model_gdf'\nplot_noise_map(final_df, noise_col='avg_noise_db')\n\n\n\n\n\n\n\n\n\nK-means cluster analysis\n\n\nCode\ncluster_features = ['Log_Noise_Exposure', 'Log_Income', 'Pct_Poverty', 'Pct_Black', 'Pct_Degree']\nX_cluster = final_df[cluster_features].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\n\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_scaled)\n\nX_cluster['Cluster'] = clusters\nX_cluster['Cluster_Name'] = X_cluster['Cluster'].map({\n    0: 'Group A', 1: 'Group B', 2: 'Group C'\n})\n\n\n\n\nCode\ncluster_means = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=cluster_features)\ncluster_means['Cluster'] = [0, 1, 2]\nplt.figure(figsize=(10, 5))\nsns.heatmap(\n    cluster_means.set_index('Cluster').T, \n    annot=True, \n    cmap='RdBu_r', \n    fmt='.2f',\n    center=cluster_means.mean().mean()\n)\nplt.title(\"Cluster Profiles: Mean Values of Each Group\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npca = PCA(n_components=2)\ncoords = pca.fit_transform(X_scaled)\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=coords[:,0], y=coords[:,1], hue=X_cluster['Cluster'], palette='viridis', s=100, alpha=0.8)\nplt.title(\"County Clusters (PCA Projection)\")\nplt.xlabel(\"Principal Component 1 (Likely SES Status)\")\nplt.ylabel(\"Principal Component 2 (Likely Urban/Noise)\")\nplt.legend(title='Cluster ID')\nplt.show()\n\n\n\n\n\n\n\n\n\nINTERACTION EFFECTS, Does Poverty amplify the harm of Noise?\n\n\nCode\ntarget_health = 'Depression among adults' # Example outcome\n\ndf_int = final_df[[target_health, 'Log_Noise_Exposure', 'Pct_Poverty']].dropna()\n    \n    # Create \"Interaction Term\"\n    # Interaction = Noise * Poverty\ndf_int['Interaction_Term'] = df_int['Log_Noise_Exposure'] * df_int['Pct_Poverty']\n    \n    # 2. Run Regression\nX_int = df_int[['Log_Noise_Exposure', 'Pct_Poverty', 'Interaction_Term']]\ny_int = df_int[target_health]\n    \nlm = LinearRegression()\nlm.fit(X_int, y_int)\n    \n    # 3. Visualization: Interaction Plot\n    # We split data into \"High Poverty\" vs \"Low Poverty\" counties to visualize the difference slopes\n    \n    # Define median poverty split\npov_median = df_int['Pct_Poverty'].median()\ndf_int['Poverty_Level'] = np.where(df_int['Pct_Poverty'] &gt; pov_median, 'High Poverty', 'Low Poverty')\n    \nplt.figure(figsize=(10, 6))\nsns.lmplot(\n    data=df_int, \n    x='Log_Noise_Exposure', \n    y=target_health, \n    hue='Poverty_Level',\n    palette={'High Poverty': 'red', 'Low Poverty': 'blue'},\n    height=6, aspect=1.5,\n    scatter_kws={'alpha': 0.3}\n)\nplt.title(f\"Interaction Effect: Noise vs {target_health}\\nby Poverty Level\")\nplt.xlabel(\"Log Noise Exposure\")\nplt.ylabel(f\"Prevalence: {target_health}\")\nplt.show()\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[175], line 3\n      1 target_health = 'Depression among adults' # Example outcome\n----&gt; 3 df_int = final_df[[outcomes_of_interest, 'Log_Noise_Exposure', 'Pct_Poverty']].dropna()\n      5     # Create \"Interaction Term\"\n      6     # Interaction = Noise * Poverty\n      7 df_int['Interaction_Term'] = df_int['Log_Noise_Exposure'] * df_int['Pct_Poverty']\n\nFile f:\\Anaconda\\envs\\geo\\Lib\\site-packages\\geopandas\\geodataframe.py:1896, in GeoDataFrame.__getitem__(self, key)\n   1890 def __getitem__(self, key):\n   1891     \"\"\"\n   1892     If the result is a column containing only 'geometry', return a\n   1893     GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\n   1894     return a GeoDataFrame.\n   1895     \"\"\"\n-&gt; 1896     result = super().__getitem__(key)\n   1897     # Custom logic to avoid waiting for pandas GH51895\n   1898     # result is not geometry dtype for multi-indexes\n   1899     if (\n   1900         pd.api.types.is_scalar(key)\n   1901         and key == \"\"\n   (...)   1904         and not is_geometry_type(result)\n   1905     ):\n\nFile f:\\Anaconda\\envs\\geo\\Lib\\site-packages\\pandas\\core\\frame.py:4119, in DataFrame.__getitem__(self, key)\n   4117     if is_iterator(key):\n   4118         key = list(key)\n-&gt; 4119     indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n   4121 # take() does not accept boolean indexers\n   4122 if getattr(indexer, \"dtype\", None) == bool:\n\nFile f:\\Anaconda\\envs\\geo\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6207, in Index._get_indexer_strict(self, key, axis_name)\n   6204     keyarr = com.asarray_tuplesafe(keyarr)\n   6206 if self._index_as_unique:\n-&gt; 6207     indexer = self.get_indexer_for(keyarr)\n   6208     keyarr = self.reindex(keyarr)[0]\n   6209 else:\n\nFile f:\\Anaconda\\envs\\geo\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6194, in Index.get_indexer_for(self, target)\n   6176 \"\"\"\n   6177 Guaranteed return of an indexer even when non-unique.\n   6178 \n   (...)   6191 array([0, 2])\n   6192 \"\"\"\n   6193 if self._index_as_unique:\n-&gt; 6194     return self.get_indexer(target)\n   6195 indexer, _ = self.get_indexer_non_unique(target)\n   6196 return indexer\n\nFile f:\\Anaconda\\envs\\geo\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3960, in Index.get_indexer(self, target, method, limit, tolerance)\n   3955     target = target.astype(dtype, copy=False)\n   3956     return this._get_indexer(\n   3957         target, method=method, limit=limit, tolerance=tolerance\n   3958     )\n-&gt; 3960 return self._get_indexer(target, method, limit, tolerance)\n\nFile f:\\Anaconda\\envs\\geo\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3987, in Index._get_indexer(self, target, method, limit, tolerance)\n   3984     else:\n   3985         tgt_values = target._get_engine_target()\n-&gt; 3987     indexer = self._engine.get_indexer(tgt_values)\n   3989 return ensure_platform_int(indexer)\n\nFile pandas/_libs/index.pyx:351, in pandas._libs.index.IndexEngine.get_indexer()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7139, in pandas._libs.hashtable.PyObjectHashTable.lookup()\n\nTypeError: unhashable type: 'list'\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nprint(\"\\n--- PHASE 3: MULTIPLE LOGISTIC REGRESSION ---\")\nprint(\"(Predicting: Is a county 'High Risk' or 'Low Risk' for this outcome?)\")\n\n# 1. Define Features & Outcomes\nfeatures = ['Log_Noise_Exposure', 'Log_Income', 'Pct_Poverty', 'Pct_Black', 'Pct_Degree']\n\n\nresults_logistic = []\n\nfor outcome in outcomes_of_interest:\n    if outcome not in final_df.columns:\n        continue\n        \n    # 2. Prepare Data\n    df_mod = final_df.dropna(subset=features + [outcome]).copy()\n    \n    # --- CRITICAL STEP: BINARIZATION ---\n    # Convert continuous % to Binary (0 or 1) based on the Median\n    median_val = df_mod[outcome].median()\n    df_mod['Target_Binary'] = (df_mod[outcome] &gt; median_val).astype(int)\n    \n    X = df_mod[features]\n    y = df_mod['Target_Binary']\n    \n    # 3. Split Data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    # Scale Features (Required for Logistic Regression regularization)\n    scaler = StandardScaler()\n    X_train_sc = scaler.fit_transform(X_train)\n    X_test_sc = scaler.transform(X_test)\n    \n    # 4. Train Logistic Regression\n    # C=1.0 is standard regularization. solver='liblinear' is good for smaller datasets.\n    log_reg = LogisticRegression(C=1.0, solver='liblinear', random_state=42)\n    log_reg.fit(X_train_sc, y_train)\n    \n    # 5. Evaluate\n    y_pred = log_reg.predict(X_test_sc)\n    acc = accuracy_score(y_test, y_pred)\n    \n    # 6. Extract Odds Ratios\n    # Coefs are log-odds; exponentiate them to get Odds Ratios\n    # Index 0 corresponds to 'Log_Noise_Exposure'\n    noise_log_odds = log_reg.coef_[0][0] \n    noise_odds_ratio = np.exp(noise_log_odds)\n    \n    results_logistic.append({\n        'Health Outcome': outcome.split(\" among\")[0],\n        'Accuracy': acc,\n        'Noise Odds Ratio': noise_odds_ratio,\n        'Median_Cutoff': median_val\n    })\n    \n    print(f\"\\nOutcome: {outcome.split(' among')[0]}\")\n    print(f\"   &gt; Cutoff (Median %): {median_val:.1f}%\")\n    print(f\"   &gt; Accuracy: {acc:.2%}\")\n    print(f\"   &gt; Noise Odds Ratio: {noise_odds_ratio:.3f}\")\n    print(f\"     (OR &gt; 1.00 means higher noise increases risk)\")\n\n# ==========================================\n# VISUALIZATION: ODDS RATIOS\n# ==========================================\nif results_logistic:\n    res_df = pd.DataFrame(results_logistic)\n\n    plt.figure(figsize=(10, 6))\n    \n    # Plot Odds Ratios\n    sns.barplot(data=res_df, x='Noise Odds Ratio', y='Health Outcome', palette='Reds')\n    \n    # Add reference line at OR = 1 (No Effect)\n    plt.axvline(1.0, color='black', linestyle='--', linewidth=2, label='No Effect (OR=1.0)')\n    plt.legend()\n    \n    plt.title(\"Impact of Noise on High-Risk Health Outcomes (Odds Ratios)\")\n    plt.xlabel(\"Odds Ratio (Values &gt; 1.0 indicate higher risk)\")\n    \n    # Annotate values\n    for index, row in res_df.iterrows():\n        plt.text(row['Noise Odds Ratio'] + 0.02, index, f\"{row['Noise Odds Ratio']:.2f}\", va='center')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n--- PHASE 3: MULTIPLE LOGISTIC REGRESSION ---\n(Predicting: Is a county 'High Risk' or 'Low Risk' for this outcome?)\n\nOutcome: Depression\n   &gt; Cutoff (Median %): 23.7%\n   &gt; Accuracy: 88.89%\n   &gt; Noise Odds Ratio: 1.097\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: High blood pressure\n   &gt; Cutoff (Median %): 31.3%\n   &gt; Accuracy: 82.54%\n   &gt; Noise Odds Ratio: 0.728\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Stroke\n   &gt; Cutoff (Median %): 3.3%\n   &gt; Accuracy: 76.19%\n   &gt; Noise Odds Ratio: 1.191\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Arthritis\n   &gt; Cutoff (Median %): 27.7%\n   &gt; Accuracy: 73.02%\n   &gt; Noise Odds Ratio: 0.892\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Cancer (non-skin) or melanoma\n   &gt; Cutoff (Median %): 8.4%\n   &gt; Accuracy: 80.95%\n   &gt; Noise Odds Ratio: 1.045\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Chronic obstructive pulmonary disease\n   &gt; Cutoff (Median %): 7.2%\n   &gt; Accuracy: 84.13%\n   &gt; Noise Odds Ratio: 0.990\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Cognitive disability\n   &gt; Cutoff (Median %): 13.5%\n   &gt; Accuracy: 71.43%\n   &gt; Noise Odds Ratio: 1.154\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Coronary heart disease\n   &gt; Cutoff (Median %): 6.8%\n   &gt; Accuracy: 80.95%\n   &gt; Noise Odds Ratio: 1.170\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Current asthma\n   &gt; Cutoff (Median %): 10.9%\n   &gt; Accuracy: 77.78%\n   &gt; Noise Odds Ratio: 1.121\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Diagnosed diabetes\n   &gt; Cutoff (Median %): 9.9%\n   &gt; Accuracy: 80.95%\n   &gt; Noise Odds Ratio: 0.954\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Fair or poor self-rated health status\n   &gt; Cutoff (Median %): 16.5%\n   &gt; Accuracy: 76.19%\n   &gt; Noise Odds Ratio: 1.172\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Frequent mental distress\n   &gt; Cutoff (Median %): 17.1%\n   &gt; Accuracy: 84.13%\n   &gt; Noise Odds Ratio: 1.055\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Hearing disability\n   &gt; Cutoff (Median %): 7.0%\n   &gt; Accuracy: 73.02%\n   &gt; Noise Odds Ratio: 0.903\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: High cholesterol\n   &gt; Cutoff (Median %): 32.5%\n   &gt; Accuracy: 44.44%\n   &gt; Noise Odds Ratio: 0.947\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Mobility disability\n   &gt; Cutoff (Median %): 12.6%\n   &gt; Accuracy: 80.95%\n   &gt; Noise Odds Ratio: 1.113\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Obesity\n   &gt; Cutoff (Median %): 34.1%\n   &gt; Accuracy: 88.89%\n   &gt; Noise Odds Ratio: 0.721\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Vision disability\n   &gt; Cutoff (Median %): 4.8%\n   &gt; Accuracy: 77.78%\n   &gt; Noise Odds Ratio: 1.345\n     (OR &gt; 1.00 means higher noise increases risk)"
  },
  {
    "objectID": "cleaned_code.html",
    "href": "cleaned_code.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\n\nCode\n# --- IMPORTS ---\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport rasterio\nimport subprocess\nfrom rasterstats import zonal_stats\nfrom sodapy import Socrata\nfrom shapely.geometry import shape\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport warnings\n\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")\n\n\n\n\nCode\n#getting the noise data\ndata_dir = \"data/CONUS_rail_road_and_aviation_noise_2020/State_rasters/\"\nsearch_pattern = os.path.join(data_dir, \"*_rail_road_and_aviation_noise_2020.tif\")\noutput_vrt = os.path.join(data_dir, \"CONUS_merged_noise_map_2020.vrt\")\n\ntif_files = glob.glob(search_pattern)\n\nif not tif_files:\n    print(f\"--- ERROR ---\")\n    print(f\"No .tif files were found at: {search_pattern}\")\nelse:\n    print(f\"Found {len(tif_files)} state raster files to link.\")\n    \n    command_list = [\"gdalbuildvrt\", output_vrt] + tif_files\n    \n    print(\"Building VRT file...\")\n    \n    try:\n        subprocess.run(command_list, check=True, shell=True)\n        \n        print(f\"\\n--- Success! ---\")\n        print(f\"Virtual Raster .vrt file created at:\")\n        print(output_vrt)\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"\\n--- GDAL ERROR ---\")\n        print(f\"The command failed: {e}\")\n        print(\"Please ensure GDAL is installed in your active Conda environment.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n\nFound 49 state raster files to link.\nBuilding VRT file...\n\n--- Success! ---\nVirtual Raster .vrt file created at:\ndata/CONUS_rail_road_and_aviation_noise_2020/State_rasters/CONUS_merged_noise_map_2020.vrt\n\n\n\n\nCode\ndata_dir = \"data/CONUS_rail_road_and_aviation_noise_2020/State_rasters/\"\nNOISE_RASTER_PATH = os.path.join(data_dir, \"CONUS_merged_noise_map_2020.vrt\")\nCDC_APP_TOKEN = \"ReEZWmd3hqYSQ879UCZyhtP1T\" \n\n\n\n\nCode\n# --- HELPER CLASS ---\nclass DataPipeline:\n    \"\"\"\n    Handles data ingestion and geometry processing for CDC and Census data.\n    \"\"\"\n    def __init__(self, cdc_token, noise_path):\n        self.cdc_token = cdc_token\n        self.noise_path = noise_path\n        self.NE_STATES = ['PA', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'VT', 'ME']\n\n    def get_cdc_data(self, limit=50000):\n        \"\"\"Fetches and cleans CDC PLACES data.\"\"\"\n        print(f\"Fetching {limit} records from CDC API...\")\n        client = Socrata(\"data.cdc.gov\", self.cdc_token)\n        dataset_id = \"swc5-untb\"\n        results = client.get(dataset_id, limit=limit)\n        \n        gdf = gpd.GeoDataFrame.from_records(results)\n        \n        # Geometry processing\n        gdf = gdf.dropna(subset=['geolocation'])\n        gdf['geometry'] = gdf['geolocation'].apply(lambda x: shape(x))\n        gdf = gdf.set_geometry('geometry', crs=\"EPSG:4326\")\n        \n        # Filter for NE States\n        gdf = gdf[gdf['stateabbr'].isin(self.NE_STATES)]\n        \n        # Numeric conversion\n        gdf['data_value'] = pd.to_numeric(gdf['data_value'], errors='coerce')\n        return gdf\n\n    def get_counties(self):\n        \"\"\"Fetches US County boundaries.\"\"\"\n        print(\"Downloading County Boundaries...\")\n        url = \"https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\"\n        counties = gpd.read_file(url)\n        counties = counties.to_crs(\"EPSG:4326\")\n        \n        # Create FIPS\n        counties['FIPS'] = counties['STATEFP'] + counties['COUNTYFP']\n        return counties[['FIPS', 'geometry']]\n\n# --- UTILITY FUNCTIONS ---\ndef calculate_vif(df):\n    \"\"\"Calculates Variance Inflation Factor to check for multicollinearity.\"\"\"\n    # Handle infinite/nan values just in case\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n    return vif_data.sort_values('VIF', ascending=False)\n\ndef plot_correlation_matrix(df, title=\"Correlation Matrix\"):\n    \"\"\"Plots a publication-quality heatmap.\"\"\"\n    plt.figure(figsize=(10, 8))\n    corr = df.corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    sns.heatmap(corr, mask=mask, cmap='RdBu_r', center=0, square=True, linewidths=.5)\n    plt.title(title, fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n# Initialize Pipeline\npipeline = DataPipeline(CDC_APP_TOKEN, NOISE_RASTER_PATH)\n\n\n\n\nCode\n# 1. Load Data\ncdc_gdf = pipeline.get_cdc_data(limit=250000)\ncounties_gdf = pipeline.get_counties()\n\n# 2. Pivot CDC Data (Wide Format)\n# We need rows = Counties, Columns = Health Outcomes + SES\nprint(\"Pivoting CDC data...\")\nhealth_pivot = cdc_gdf.pivot_table(\n    index=['locationid', 'locationname', 'stateabbr'],\n    columns='measure',\n    values='data_value',\n    aggfunc='mean'\n).reset_index()\n\n\nFetching 250000 records from CDC API...\nDownloading County Boundaries...\nPivoting CDC data...\n\n\n\n\nCode\ncounties_gdf.to_file(\"F:\\GitHub\\MUSA550-Final-Noise_on_health\\MUSA550-Final-Noise_on_health\\website\\data-analysis_files\\counties_gdf.geojson\", driver = 'GeoJSON')\n\n\n\n\nCode\n# 3. Merge with Geometries\nmodel_gdf = counties_gdf.merge(health_pivot, left_on='FIPS', right_on='locationid', how='inner')\n\n\n\n\nCode\nmodel_gdf = model_gdf.set_geometry('geometry')\n\nprint(f\"Final Dataset: {len(model_gdf)} counties ready for analysis.\")\n\n\nFinal Dataset: 218 counties ready for analysis.\n\n\n\n\nCode\nimport rasterio\nfrom rasterio.mask import mask\nfrom rasterio.plot import show\nfrom rasterio.windows import from_bounds\nfrom rasterio.enums import Resampling\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import mapping\nimport numpy as np\n\nDOWNSAMPLE_FACTOR = 0.1\n\nwith rasterio.open(NOISE_RASTER_PATH) as src:\n        model_gdf = model_gdf.to_crs(src.crs)\n        minx, miny, maxx, maxy = model_gdf.total_bounds\n        window = from_bounds(minx, miny, maxx, maxy, src.transform)\n        new_height = int(window.height * DOWNSAMPLE_FACTOR)\n        new_width = int(window.width * DOWNSAMPLE_FACTOR)\n        out_image = src.read(\n        1,\n        window=window,\n        out_shape=(new_height, new_width),\n        resampling=Resampling.bilinear\n    )\n\nnoise_data_masked = np.ma.masked_less_equal(out_image, 0)\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\nimg = ax.imshow(\n    noise_data_masked, \n    cmap='inferno_r', \n    extent=(minx, maxx, miny, maxy),\n    origin='upper'\n)\n\n# Formatting\nplt.colorbar(img, ax=ax, label=\"Noise Level (dB)\", fraction=0.01, pad=0.04)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nax.set_axis_off()\nfig.patch.set_facecolor('none')\nax.patch.set_facecolor('none')\nplt.tight_layout\nplt.show()\n\n\n\n\n\n\n\n\n\nFirst attempted regression\n\n\nCode\n# 1. Calculate Naive Zonal Statistics (Arithmetic Mean)\n# Note: This is physically incorrect for decibels, but demonstrated for baseline comparison.\nwith rasterio.open(NOISE_RASTER_PATH) as src:\n    target_crs = src.crs\n    nodata_val = src.nodata\n\nmodel_gdf = model_gdf.to_crs(target_crs)\n\nstats_arithmetic = zonal_stats(\n    model_gdf,\n    NOISE_RASTER_PATH,\n    stats=['mean'],\n    all_touched=True,\n    band=1,\n    nodata=nodata_val\n)\n\nresult_gdf = pd.DataFrame(stats_arithmetic)\nresult_gdf['locationid'] = model_gdf['locationid'].values\nresult_gdf = result_gdf.rename(columns={'mean': 'avg_noise_db'})\nresult_gdf = result_gdf.dropna(subset=['avg_noise_db'])\n\n\n\n\nCode\nmodel_gdf = model_gdf.merge(\n    result_gdf,\n    on='locationid',\n    how='left'\n)\n\n\n\n\nCode\n# TARGET (y) = Noise \n# FEATURES (X) = All Health Outcomes\ntarget = 'avg_noise_db'\nfeatures = [col for col in model_gdf.columns if col not in ['FIPS', 'geometry', 'locationid', 'locationname', 'stateabbr', 'naive_noise_db']]\n\nX = model_gdf[features].dropna()\ny = model_gdf.loc[X.index, target]\n\n\n\n\nCode\n# Run Random Forest\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nrf_naive = RandomForestRegressor(random_state=42)\nrf_naive.fit(X_train, y_train)\n\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n1.0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\n# Diagnostic Outputs\nprint(f\"\\n Model R² Score: {rf_naive.score(X_test, y_test):.4f}\")\n\nvif_df = calculate_vif(X)\nprint(vif_df.head(10))\n\n# Plot Correlation\nplot_correlation_matrix(X)\n\n\n\n Model R² Score: 0.9620\n                                              feature          VIF\n9                 Coronary heart disease among adults  8052.084383\n38  Visited dentist or dental clinic in the past y...  7944.017177\n32                  Self-care disability among adults  7598.641118\n35  Taking medicine to control high blood pressure...  7556.890581\n19            Frequent physical distress among adults  7468.313397\n39  Visits to doctor for routine checkup within th...  6741.279607\n37                     Vision disability among adults  6636.673746\n34                                Stroke among adults  6292.508869\n15  Fair or poor self-rated health status among ad...  6247.298957\n18              Frequent mental distress among adults  5350.840512\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 1. DEFINE CENSUS FETCHING FUNCTION\n# ---------------------------------------------------------\ndef fetch_census_data_standalone():\n    \"\"\"\n    Fetches 2023 ACS 5-Year Data via US Census API for Northeast States.\n    Returns a cleaned DataFrame ready for merging.\n    \"\"\"\n    print(\"Fetching US Census ACS Data (Income, Poverty, Race, Education)...\")\n    \n    # NE State FIPS: CT, ME, MA, NH, NJ, NY, PA, RI, VT\n    ne_fips = [\"09\", \"23\", \"25\", \"33\", \"34\", \"36\", \"42\", \"44\", \"50\"]\n    \n    # Map API variables to readable names\n    vars_map = {\n        \"B01003_001E\": \"Total_Pop\",\n        \"B19013_001E\": \"Median_Income\",\n        \"B17001_002E\": \"Poverty_Count\",\n        \"B03002_003E\": \"White_Count\",\n        \"B03002_004E\": \"Black_Count\",\n        \"B15003_022E\": \"Bachelor_Degree_Count\"\n    }\n    \n    var_string = \",\".join(vars_map.keys())\n    dfs = []\n    \n  \n    for fips in ne_fips:\n        url = f\"https://api.census.gov/data/2023/acs/acs5?get=NAME,{var_string}&for=county:*&in=state:{fips}\"\n        try:\n            df = pd.read_json(url)\n            df.columns = df.iloc[0]\n            df = df[1:]\n            dfs.append(df)\n        except Exception as e:\n                print(f\"Error fetching state {fips}: {e}\")\n        \n    if not dfs:\n         raise ValueError(\"Failed to fetch Census data.\")\n    \n    census = pd.concat(dfs, ignore_index=True)\n    \n    # Create 5-digit FIPS for merging (State + County)\n    census['FIPS'] = census['state'] + census['county']\n    \n    # Rename columns\n    census = census.rename(columns=vars_map)\n    \n    # Convert to Numeric\n    for col in vars_map.values():\n        census[col] = pd.to_numeric(census[col], errors='coerce')\n        \n    census['Pct_Poverty'] = (census['Poverty_Count'] / census['Total_Pop']) * 100\n    census['Pct_White'] = (census['White_Count'] / census['Total_Pop']) * 100\n    census['Pct_Black'] = (census['Black_Count'] / census['Total_Pop']) * 100\n    census['Pct_Degree'] = (census['Bachelor_Degree_Count'] / census['Total_Pop']) * 100\n    census['Log_Income'] = np.log1p(census['Median_Income'])\n    \n    # Select final columns\n    return census[['FIPS', 'Total_Pop', 'Log_Income', 'Pct_Poverty', 'Pct_White', 'Pct_Black', 'Pct_Degree']]\n\n\n\n\nCode\n#%pip install pyarrow\n#import pyarrow\n\n\nCollecting pyarrow\n  Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\nDownloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n   ----- ---------------------------------- 3.9/28.0 MB 22.7 MB/s eta 0:00:02\n   ------------ --------------------------- 8.9/28.0 MB 23.9 MB/s eta 0:00:01\n   ------------------- -------------------- 13.9/28.0 MB 23.8 MB/s eta 0:00:01\n   --------------------------- ------------ 19.1/28.0 MB 24.6 MB/s eta 0:00:01\n   --------------------------------- ------ 23.6/28.0 MB 24.1 MB/s eta 0:00:01\n   ---------------------------------------  27.8/28.0 MB 24.0 MB/s eta 0:00:01\n   ---------------------------------------- 28.0/28.0 MB 21.9 MB/s  0:00:01\nInstalling collected packages: pyarrow\nSuccessfully installed pyarrow-22.0.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\ncensus_df = fetch_census_data_standalone()\n\n\nFetching US Census ACS Data (Income, Poverty, Race, Education)...\n\n\n\n\nCode\ncensus_df.to_csv(\"F:\\GitHub\\MUSA550-Final-Noise_on_health\\MUSA550-Final-Noise_on_health\\website\\data-analysis_files\\census_df.csv\", index=False)\n\n\n\n\nCode\n# Ensure key types match (strings)\nmodel_gdf['FIPS'] = model_gdf['locationid'].astype(str).str.zfill(5)\ncensus_df['FIPS'] = census_df['FIPS'].astype(str).str.zfill(5)\n\n# Perform Merge\nfinal_df = model_gdf.merge(census_df, left_on='FIPS', right_on='FIPS', how='left')\n\n# Physics Correction: Log-Transform the Noise (Decibels are logarithmic, but health impacts often scale linearly with Log(Energy))\n# Note: We use the 'avg_noise_db' you calculated in your previous cells\nfinal_df['Log_Noise_Exposure'] = np.log1p(final_df['avg_noise_db'])\n\nprint(f\"Final Dataset with Census Data: {len(final_df)} counties.\")\n\n\nFinal Dataset with Census Data: 218 counties.\n\n\nCT decided to change their county shape in addition to the total number of county’s they have from 8 to 9. This change happened on August 2024, which means there is no way to get ACS 5 data from CT as the earliest ACS 5 data we can get is in 2023.\nThe two options I have is either to use the less reliable ACS 1 year data’s, or I\n\n\nCode\n# 3. RUN CORRECTED REGRESSION LOOP\n# ---------------------------------------------------------\nprint(\"\\n--- RUNNING ROBUST RIDGE REGRESSIONS ---\")\n\n# Define inputs (X) - Socioeconomics + Noise\nfeatures = ['Log_Noise_Exposure', 'Log_Income', 'Pct_Poverty', 'Pct_Black', 'Pct_Degree']\n\n# Define Health Outcomes (y)\noutcomes_of_interest = [\n    'Depression among adults',\n    'High blood pressure among adults', \n    'Stroke among adults',\n    'Sleep duration &lt; 7 hours among adults aged &gt;=18 years', \n    'Arthritis among adults', \n    'Cancer (non-skin) or melanoma among adults', \n    'Chronic obstructive pulmonary disease among adults', \n    'Cognitive disability among adults', \n    'Coronary heart disease among adults', \n    'Current asthma among adults', \n    'Diagnosed diabetes among adults', \n    'Fair or poor self-rated health status among adults', \n    'Frequent mental distress among adults',\n    'Hearing disability among adults',\n    'High cholesterol among adults who have ever been screened', \n    'Mobility disability among adults', \n    'Obesity among adults', \n    'Vision disability among adults'\n]\n\nresults_list = []\n\nfor outcome in outcomes_of_interest:\n    # Skip if outcome not in data\n    if outcome not in final_df.columns:\n        continue\n        \n    # Drop NaNs for this specific combination\n    df_mod = final_df.dropna(subset=features + [outcome])\n    \n    if len(df_mod) &lt; 10:\n        print(f\"Skipping {outcome}: Not enough data.\")\n        continue\n\n    X = df_mod[features]\n    y = df_mod[outcome]\n    \n    # Split Data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    # Scale Data (Important for comparing coefficients of Noise vs Income)\n    scaler = StandardScaler()\n    X_train_sc = scaler.fit_transform(X_train)\n    X_test_sc = scaler.transform(X_test)\n    \n    # Ridge Regression (Handles multicollinearity between Income/Poverty better than LinearRegression)\n    model = Ridge(alpha=1.0)\n    model.fit(X_train_sc, y_train)\n    \n    # Calculate R2\n    r2 = r2_score(y_test, model.predict(X_test_sc))\n    \n    # Extract Noise Coefficient (Index 0 is 'Log_Noise_Exposure')\n    noise_coef = model.coef_[0]\n    \n    results_list.append({\n        'Health Outcome': outcome.split(\" among\")[0],\n        'R2 Score': r2,\n        'Noise Impact (Beta)': noise_coef\n    })\n    \n    print(f\"Outcome: {outcome[:15]}... | R²: {r2:.3f} | Noise Beta: {noise_coef:.3f}\")\n\n\n\n--- RUNNING ROBUST RIDGE REGRESSIONS ---\nOutcome: Depression amon... | R²: 0.605 | Noise Beta: 0.037\nOutcome: High blood pres... | R²: 0.607 | Noise Beta: -0.120\nOutcome: Stroke among ad... | R²: 0.731 | Noise Beta: -0.002\nOutcome: Arthritis among... | R²: 0.639 | Noise Beta: -0.101\nOutcome: Cancer (non-ski... | R²: 0.714 | Noise Beta: -0.007\nOutcome: Chronic obstruc... | R²: 0.738 | Noise Beta: 0.017\nOutcome: Cognitive disab... | R²: 0.603 | Noise Beta: -0.009\nOutcome: Coronary heart ... | R²: 0.649 | Noise Beta: -0.017\nOutcome: Current asthma ... | R²: 0.333 | Noise Beta: 0.031\nOutcome: Diagnosed diabe... | R²: 0.578 | Noise Beta: -0.028\nOutcome: Fair or poor se... | R²: 0.638 | Noise Beta: -0.035\nOutcome: Frequent mental... | R²: 0.697 | Noise Beta: -0.015\nOutcome: Hearing disabil... | R²: 0.737 | Noise Beta: -0.001\nOutcome: High cholestero... | R²: -0.023 | Noise Beta: -0.128\nOutcome: Mobility disabi... | R²: 0.691 | Noise Beta: 0.013\nOutcome: Obesity among a... | R²: 0.595 | Noise Beta: -0.274\nOutcome: Vision disabili... | R²: 0.736 | Noise Beta: -0.007\n\n\n\n\nCode\nvif_features = ['Log_Noise_Exposure', 'Log_Income', 'Pct_Poverty', 'Pct_Black', 'Pct_Degree']\nX_vif = final_df[vif_features].dropna()\nX_vif['const'] = 1\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = X_vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) \n                   for i in range(len(X_vif.columns))]\nvif_display = vif_data[vif_data[\"Feature\"] != 'const'].sort_values('VIF', ascending=False)\nprint(vif_display)\n\n\n              Feature       VIF\n1          Log_Income  4.571799\n2         Pct_Poverty  3.019654\n4          Pct_Degree  2.863944\n3           Pct_Black  1.609137\n0  Log_Noise_Exposure  1.058018\n\n\nAll features have a VIF below 5, showing low correlation between features\n\n\nCode\ncorr_matrix = final_df[vif_features].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    corr_matrix, \n    annot=True,       # Show the numbers in the boxes\n    fmt=\".2f\",        # Round to 2 decimal places\n    cmap='RdBu_r',    # Red/Blue diverging colormap (Red=Pos, Blue=Neg)\n    center=0,         # Center the colormap at 0\n    vmin=-1, vmax=1,  # Fix scale from -1 to 1\n    square=True,      # Force square aspect ratio\n    linewidths=.5,    # Add lines between squares\n    cbar_kws={\"shrink\": .8}\n)\nplt.title('Correlation Matrix of Regression Features', fontsize=16)\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 4. VISUALIZE RESULTS\n# ---------------------------------------------------------\nif results_list:\n    res_df = pd.DataFrame(results_list)\n\n    plt.figure(figsize=(12, 5))\n\n    # Plot 1: R2 Scores\n    plt.subplot(1, 2, 1)\n    sns.barplot(data=res_df, x='R2 Score', y='Health Outcome', palette='viridis')\n    plt.title(\"Model Fit (R²)\")\n    plt.xlabel(\"Variance Explained (0-1)\")\n    plt.xlim(0, 1)\n\n    # Plot 2: Noise Coefficients\n    plt.subplot(1, 2, 2)\n    sns.barplot(data=res_df, x='Noise Impact (Beta)', y='Health Outcome', palette='magma')\n    plt.title(\"Standardized Impact of Noise\")\n    plt.xlabel(\"Beta Coefficient (Positive = Harmful)\")\n    plt.axvline(0, color='black', linestyle='--')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No matching health outcomes found to plot.\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ==========================================\n# PHASE 3: SPATIAL VISUALIZATION\n# ==========================================\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef plot_noise_map(gdf, noise_col='avg_noise_db', title='Average Transportation Noise by County'):\n    \"\"\"\n    Plots a choropleth map of noise levels.\n    \"\"\"\n    # 1. Prepare Data (Ensure it's a GeoDataFrame)\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        print(\"Converting to GeoDataFrame...\")\n        gdf = gpd.GeoDataFrame(gdf, geometry='geometry')\n    \n    # Check if column exists\n    if noise_col not in gdf.columns:\n        # Fallback to other likely names from previous steps\n        options = ['Noise_Mean_dB', 'noise_db', 'Log_Noise_Exposure']\n        for opt in options:\n            if opt in gdf.columns:\n                noise_col = opt\n                print(f\"Using '{noise_col}' for plotting.\")\n                break\n        else:\n            print(f\"Error: Could not find noise column '{noise_col}' in dataset.\")\n            return\n\n    # 2. Setup Plot\n    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\n    # 3. Plot\n    gdf.plot(\n        column=noise_col,\n        ax=ax,\n        legend=True,\n        cax=cax,\n        cmap='inferno_r', \n        legend_kwds={'label': \"Noise Level (dB)\"},\n        missing_kwds={'color': 'lightgrey', 'label': 'Missing Data'}\n    )\n\n    # 4. Formatting\n    ax.set_title(title, fontsize=16, fontweight='bold')\n    ax.set_axis_off() \n    \n    plt.tight_layout()\n    plt.show()\n\n# --- EXECUTE PLOT ---\n# Try to plot using the 'final_df' from Phase 2, or fallback to 'model_gdf'\nplot_noise_map(final_df, noise_col='avg_noise_db')\n\n\n\n\n\n\n\n\n\nK-means cluster analysis\n\n\nCode\ncluster_features = ['Log_Noise_Exposure', 'Log_Income', 'Pct_Poverty', 'Pct_Black', 'Pct_Degree']\nX_cluster = final_df[cluster_features].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\n\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_scaled)\n\nX_cluster['Cluster'] = clusters\nX_cluster['Cluster_Name'] = X_cluster['Cluster'].map({\n    0: 'Group A', 1: 'Group B', 2: 'Group C'\n})\n\n\n\n\nCode\ncluster_means = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=cluster_features)\ncluster_means['Cluster'] = [0, 1, 2]\nplt.figure(figsize=(10, 5))\nsns.heatmap(\n    cluster_means.set_index('Cluster').T, \n    annot=True, \n    cmap='RdBu_r', \n    fmt='.2f',\n    center=cluster_means.mean().mean()\n)\nplt.title(\"Cluster Profiles: Mean Values of Each Group\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npca = PCA(n_components=2)\ncoords = pca.fit_transform(X_scaled)\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=coords[:,0], y=coords[:,1], hue=X_cluster['Cluster'], palette='viridis', s=100, alpha=0.8)\nplt.title(\"County Clusters (PCA Projection)\")\nplt.xlabel(\"Principal Component 1 (Likely SES Status)\")\nplt.ylabel(\"Principal Component 2 (Likely Urban/Noise)\")\nplt.legend(title='Cluster ID')\nplt.show()\n\n\n\n\n\n\n\n\n\nINTERACTION EFFECTS, Does Poverty amplify the harm of Noise?\n\n\nCode\ntarget_health = 'Depression among adults' # Example outcome\n\ndf_int = final_df[[target_health, 'Log_Noise_Exposure', 'Pct_Poverty']].dropna()\n    \n    # Create \"Interaction Term\"\n    # Interaction = Noise * Poverty\ndf_int['Interaction_Term'] = df_int['Log_Noise_Exposure'] * df_int['Pct_Poverty']\n    \n    # 2. Run Regression\nX_int = df_int[['Log_Noise_Exposure', 'Pct_Poverty', 'Interaction_Term']]\ny_int = df_int[target_health]\n    \nlm = LinearRegression()\nlm.fit(X_int, y_int)\n    \n    # 3. Visualization: Interaction Plot\n    # We split data into \"High Poverty\" vs \"Low Poverty\" counties to visualize the difference slopes\n    \n    # Define median poverty split\npov_median = df_int['Pct_Poverty'].median()\ndf_int['Poverty_Level'] = np.where(df_int['Pct_Poverty'] &gt; pov_median, 'High Poverty', 'Low Poverty')\n    \nplt.figure(figsize=(10, 6))\nsns.lmplot(\n    data=df_int, \n    x='Log_Noise_Exposure', \n    y=target_health, \n    hue='Poverty_Level',\n    palette={'High Poverty': 'red', 'Low Poverty': 'blue'},\n    height=6, aspect=1.5,\n    scatter_kws={'alpha': 0.3}\n)\nplt.title(f\"Interaction Effect: Noise vs {target_health}\\nby Poverty Level\")\nplt.xlabel(\"Log Noise Exposure\")\nplt.ylabel(f\"Prevalence: {target_health}\")\nplt.show()\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nprint(\"\\n--- PHASE 3: MULTIPLE LOGISTIC REGRESSION ---\")\nprint(\"(Predicting: Is a county 'High Risk' or 'Low Risk' for this outcome?)\")\n\n# 1. Define Features & Outcomes\nfeatures = ['Log_Noise_Exposure', 'Log_Income', 'Pct_Poverty', 'Pct_Black', 'Pct_Degree']\n\n\nresults_logistic = []\n\nfor outcome in outcomes_of_interest:\n    if outcome not in final_df.columns:\n        continue\n        \n    # 2. Prepare Data\n    df_mod = final_df.dropna(subset=features + [outcome]).copy()\n    \n    # --- CRITICAL STEP: BINARIZATION ---\n    # Convert continuous % to Binary (0 or 1) based on the Median\n    median_val = df_mod[outcome].median()\n    df_mod['Target_Binary'] = (df_mod[outcome] &gt; median_val).astype(int)\n    \n    X = df_mod[features]\n    y = df_mod['Target_Binary']\n    \n    # 3. Split Data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    # Scale Features (Required for Logistic Regression regularization)\n    scaler = StandardScaler()\n    X_train_sc = scaler.fit_transform(X_train)\n    X_test_sc = scaler.transform(X_test)\n    \n    # 4. Train Logistic Regression\n    # C=1.0 is standard regularization. solver='liblinear' is good for smaller datasets.\n    log_reg = LogisticRegression(C=1.0, solver='liblinear', random_state=42)\n    log_reg.fit(X_train_sc, y_train)\n    \n    # 5. Evaluate\n    y_pred = log_reg.predict(X_test_sc)\n    acc = accuracy_score(y_test, y_pred)\n    \n    # 6. Extract Odds Ratios\n    # Coefs are log-odds; exponentiate them to get Odds Ratios\n    # Index 0 corresponds to 'Log_Noise_Exposure'\n    noise_log_odds = log_reg.coef_[0][0] \n    noise_odds_ratio = np.exp(noise_log_odds)\n    \n    results_logistic.append({\n        'Health Outcome': outcome.split(\" among\")[0],\n        'Accuracy': acc,\n        'Noise Odds Ratio': noise_odds_ratio,\n        'Median_Cutoff': median_val\n    })\n    \n    print(f\"\\nOutcome: {outcome.split(' among')[0]}\")\n    print(f\"   &gt; Cutoff (Median %): {median_val:.1f}%\")\n    print(f\"   &gt; Accuracy: {acc:.2%}\")\n    print(f\"   &gt; Noise Odds Ratio: {noise_odds_ratio:.3f}\")\n    print(f\"     (OR &gt; 1.00 means higher noise increases risk)\")\n\n# ==========================================\n# VISUALIZATION: ODDS RATIOS\n# ==========================================\nif results_logistic:\n    res_df = pd.DataFrame(results_logistic)\n\n    plt.figure(figsize=(10, 6))\n    \n    # Plot Odds Ratios\n    sns.barplot(data=res_df, x='Noise Odds Ratio', y='Health Outcome', palette='Reds')\n    \n    # Add reference line at OR = 1 (No Effect)\n    plt.axvline(1.0, color='black', linestyle='--', linewidth=2, label='No Effect (OR=1.0)')\n    plt.legend()\n    \n    plt.title(\"Impact of Noise on High-Risk Health Outcomes (Odds Ratios)\")\n    plt.xlabel(\"Odds Ratio (Values &gt; 1.0 indicate higher risk)\")\n    \n    # Annotate values\n    for index, row in res_df.iterrows():\n        plt.text(row['Noise Odds Ratio'] + 0.02, index, f\"{row['Noise Odds Ratio']:.2f}\", va='center')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n--- PHASE 3: MULTIPLE LOGISTIC REGRESSION ---\n(Predicting: Is a county 'High Risk' or 'Low Risk' for this outcome?)\n\nOutcome: Depression\n   &gt; Cutoff (Median %): 23.6%\n   &gt; Accuracy: 78.79%\n   &gt; Noise Odds Ratio: 1.133\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: High blood pressure\n   &gt; Cutoff (Median %): 31.2%\n   &gt; Accuracy: 83.33%\n   &gt; Noise Odds Ratio: 0.828\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Stroke\n   &gt; Cutoff (Median %): 3.3%\n   &gt; Accuracy: 87.88%\n   &gt; Noise Odds Ratio: 1.142\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Arthritis\n   &gt; Cutoff (Median %): 27.6%\n   &gt; Accuracy: 75.76%\n   &gt; Noise Odds Ratio: 0.814\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Cancer (non-skin) or melanoma\n   &gt; Cutoff (Median %): 8.4%\n   &gt; Accuracy: 78.79%\n   &gt; Noise Odds Ratio: 1.053\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Chronic obstructive pulmonary disease\n   &gt; Cutoff (Median %): 7.1%\n   &gt; Accuracy: 86.36%\n   &gt; Noise Odds Ratio: 1.147\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Cognitive disability\n   &gt; Cutoff (Median %): 13.5%\n   &gt; Accuracy: 78.79%\n   &gt; Noise Odds Ratio: 1.055\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Coronary heart disease\n   &gt; Cutoff (Median %): 6.8%\n   &gt; Accuracy: 80.30%\n   &gt; Noise Odds Ratio: 1.050\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Current asthma\n   &gt; Cutoff (Median %): 11.0%\n   &gt; Accuracy: 66.67%\n   &gt; Noise Odds Ratio: 1.159\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Diagnosed diabetes\n   &gt; Cutoff (Median %): 9.9%\n   &gt; Accuracy: 83.33%\n   &gt; Noise Odds Ratio: 0.844\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Fair or poor self-rated health status\n   &gt; Cutoff (Median %): 16.3%\n   &gt; Accuracy: 81.82%\n   &gt; Noise Odds Ratio: 1.101\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Frequent mental distress\n   &gt; Cutoff (Median %): 17.1%\n   &gt; Accuracy: 87.88%\n   &gt; Noise Odds Ratio: 1.083\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Hearing disability\n   &gt; Cutoff (Median %): 7.0%\n   &gt; Accuracy: 84.85%\n   &gt; Noise Odds Ratio: 0.930\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: High cholesterol\n   &gt; Cutoff (Median %): 32.5%\n   &gt; Accuracy: 51.52%\n   &gt; Noise Odds Ratio: 0.902\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Mobility disability\n   &gt; Cutoff (Median %): 12.6%\n   &gt; Accuracy: 84.85%\n   &gt; Noise Odds Ratio: 1.012\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Obesity\n   &gt; Cutoff (Median %): 33.8%\n   &gt; Accuracy: 86.36%\n   &gt; Noise Odds Ratio: 0.698\n     (OR &gt; 1.00 means higher noise increases risk)\n\nOutcome: Vision disability\n   &gt; Cutoff (Median %): 4.8%\n   &gt; Accuracy: 86.36%\n   &gt; Noise Odds Ratio: 1.269\n     (OR &gt; 1.00 means higher noise increases risk)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Transportation Noise and its Impact on Health Outcomes in the Northeast",
    "section": "",
    "text": "MUSA 5500 Geospatial Data Science in Python | Jason Fan"
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Transportation Noise and its Impact on Health Outcomes in the Northeast",
    "section": "Project Overview",
    "text": "Project Overview\nDoes the roar of a highway or the rumble of a train affect your physical and mental health? This project investigates the relationship between transportation noise pollution and various health outcomes across counties in the Northeast United States.\nBy integrating National Transportation Noise Map data with CDC PLACES health data and US Census socioeconomic indicators, we aim to uncover statistical links between our acoustic environment and our well-being.\nRead the Analysis →"
  },
  {
    "objectID": "index.html#key-findings-at-a-glance",
    "href": "index.html#key-findings-at-a-glance",
    "title": "Transportation Noise and its Impact on Health Outcomes in the Northeast",
    "section": "Key Findings at a Glance",
    "text": "Key Findings at a Glance\n\n\nThe “Expectation of Quiet”\nPsychological sensitivity to noise is linked to privilege. High-income/education groups report the highest depression rates (~24%) despite living in the quietest areas, suggesting a lower tolerance for environmental disturbance.\n\n\nThe “Cosmopolitan Buffer”\nA striking paradox exists in dense urban cores: residents report the lowest depression (~15%) yet suffer the highest cholesterol (~36%). The social vibrancy of the city protects the mind, but the noise silently taxes the body.\n\n\nThe Hidden “Body Tax”\nWhile lower-income groups report less psychological distress (likely due to habituation), their bodies keep the score. We observed a classic Environmental Justice pattern where noise exposure correlates strongly with high cholesterol (Allostatic Load) in lower-education populations."
  }
]