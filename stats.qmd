---
title: "Regression Analysis"
execute:
    warning: false
    message: false
    echo: false
---
### Ridge Regression
```{python}
#| eval: false 
#| code-summary: "regression"

# Define inputs (X) - Socioeconomics + Noise
features = ['avg_noise_db',
            'Total_Pop',
            'Log_Income',
            'Pct_Poverty', 
            'Pct_White', 
            'Pct_Black',
            'Pct_Asian', 
            'Pct_Hispanic', 
            'Pct_Degree', 
            'Pct_Senior'
            ]

# Define Health Outcomes (y)
outcomes_of_interest = [
    'Depression among adults',
    'High blood pressure among adults', 
    'Stroke among adults',
    'Sleep duration < 7 hours among adults aged >=18 years', 
    'Arthritis among adults', 
    'Cancer (non-skin) or melanoma among adults', 
    'Chronic obstructive pulmonary disease among adults', 
    'Cognitive disability among adults', 
    'Coronary heart disease among adults', 
    'Current asthma among adults', 
    'Diagnosed diabetes among adults', 
    'Fair or poor self-rated health status among adults', 
    'Frequent mental distress among adults',
    'Hearing disability among adults',
    'High cholesterol among adults who have ever been screened', 
    'Mobility disability among adults', 
    'Obesity among adults', 
    'Vision disability among adults'
]

results_list = []

for outcome in outcomes_of_interest:
    # Skip if outcome not in data
    if outcome not in final_df.columns:
        continue
        
    # Drop NaNs for this specific combination
    df_mod = final_df.dropna(subset=features + [outcome])
    
    if len(df_mod) < 10:
        print(f"Skipping {outcome}: Not enough data.")
        continue

    X = df_mod[features]
    y = df_mod[outcome]
    
    # Split Data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Scale Data (Important for comparing coefficients of Noise vs Income)
    scaler = StandardScaler()
    X_train_sc = scaler.fit_transform(X_train)
    X_test_sc = scaler.transform(X_test)
    
    # Ridge Regression (Handles multicollinearity between Income/Poverty better than LinearRegression)
    model = Ridge(alpha=1.0)
    model.fit(X_train_sc, y_train)
    
    # Calculate R2
    r2 = r2_score(y_test, model.predict(X_test_sc))
    
    # Extract Noise Coefficient (Index 0 is 'avg_noise_db')
    noise_coef = model.coef_[0]
    
    results_list.append({
        'Health Outcome': outcome.split(" among")[0],
        'R2 Score': r2,
        'Noise Impact (Beta)': noise_coef
    })
    
    print(f"Outcome: {outcome} | R²: {r2:.3f} | Noise Beta: {noise_coef:.3f}")
```

| Health Outcome | R-Squared | Noise Beta |
|:---|:---:|:---:|
| Depression | 0.696 | -0.009 |
| High blood pressure | 0.637 | -0.089 |
| Stroke | 0.804 | 0.004 |
| Arthritis | 0.717 | -0.097 |
| Cancer (non-skin) or melanoma | 0.955 | -0.012 |
| Chronic obstructive pulmonary disease | 0.753 | 0.024 |
| Cognitive disability | 0.430 | 0.029 |
| Coronary heart disease | 0.750 | -0.007 |
| Current asthma | 0.512 | 0.049 |
| Diagnosed diabetes | 0.670 | -0.011 |
| Fair or poor self-rated health status | 0.673 | 0.032 |
| Frequent mental distress | 0.706 | -0.010 |
| Hearing disability | 0.735 | 0.012 |
| High cholesterol | 0.091 | -0.119 |
| Mobility disability | 0.738 | 0.045 |
| Obesity | 0.501 | -0.210 |
| Vision disability | 0.873 | 0.027 |

: Robust Ridge Regression Results: Noise Impact on Health {#tbl-ridge-results}


```{python}
#| eval: false 
#| code-summary: "VIF"
vif_features = ['avg_noise_db',
            'Total_Pop',
            'Log_Income',
            'Pct_Poverty', 
            'Pct_White', 
            'Pct_Black',
            'Pct_Asian', 
            'Pct_Hispanic', 
            'Pct_Degree', 
            'Pct_Senior'
            ]
X_vif = final_df[vif_features].dropna()
X_vif['const'] = 1
vif_data = pd.DataFrame()
vif_data["Feature"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) 
                   for i in range(len(X_vif.columns))]
vif_display = vif_data[vif_data["Feature"] != 'const'].sort_values('VIF', ascending=False)
print(vif_display)
```

![Inital VIF](images/initial%20VIF.png)

WOW that VIF is bad! Therefore I got rid of features until it wasn't bad

```{python}
#| eval: false 
#| code-summary: "mid VIF"
vif_features = ['avg_noise_db',
            'Total_Pop',
            'Log_Income',
            'Pct_Poverty', 
            'Pct_Black',
            'Pct_Asian', 
            'Pct_Hispanic', 
            'Pct_Degree', 
            'Pct_Senior'
            ]
X_vif = final_df[vif_features].dropna()
X_vif['const'] = 1
vif_data = pd.DataFrame()
vif_data["Feature"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) 
                   for i in range(len(X_vif.columns))]
vif_display = vif_data[vif_data["Feature"] != 'const'].sort_values('VIF', ascending=False)
print(vif_display)
```
![mid vif](images/mid_vif.png)
While Log_income has a VIF above 5, I'll be getting rid of something that might be correlated to the income such as poverty. This is due to income showing a lot more about a place such as being wealthy or not, while poverty only shows one statistic. 
```{python}
#| eval: false 
#| code-summary: "clean VIF"
vif_features = ['avg_noise_db',
            'Total_Pop',
            'Log_Income',
            'Pct_Black',
            'Pct_Asian', 
            'Pct_Hispanic', 
            'Pct_Degree', 
            'Pct_Senior'
            ]
X_vif = final_df[vif_features].dropna()
X_vif['const'] = 1
vif_data = pd.DataFrame()
vif_data["Feature"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) 
                   for i in range(len(X_vif.columns))]
vif_display = vif_data[vif_data["Feature"] != 'const'].sort_values('VIF', ascending=False)
print(vif_display)
```
![Final VIF](images/good_vif.png)
Now all VIF is below 5 showing no multicollinearity, meaning that each variables are completely independent of each other. 

### P Value Test
```{python}
#| eval: false 
#| code-summary: "P value"
import statsmodels.api as sm
import pandas as pd

predictors = ['avg_noise_db',
            'Total_Pop',
            'Log_Income',
            'Pct_Black',
            'Pct_Asian', 
            'Pct_Hispanic', 
            'Pct_Degree', 
            'Pct_Senior'
            ]
for outcome_var in outcomes_of_interest:
    print(f"\n" + "="*60)
    print(f"RUNNING REGRESSION FOR: {outcome_var}")
    print("="*60)
    
    # Safety Check: Ensure the outcome exists in the dataframe
    if outcome_var not in final_df.columns:
        print(f"Skipping {outcome_var}: Column not found in dataframe.")
        continue

    # 1. Prepare Data specific to this outcome
    # We drop NA values only for the current outcome + predictors
    # This preserves data for other outcomes if this one has missing rows
    regression_df = final_df.dropna(subset=predictors + [outcome_var]).copy()

    # 2. Define X (Features) and y (Target)
    X = regression_df[predictors]
    y = regression_df[outcome_var]

    # 3. Add a Constant (Intercept)
    # Essential for OLS, otherwise it forces the line through (0,0)
    X = sm.add_constant(X)

    # 4. Fit the Model
    try:
        ols_model = sm.OLS(y, X).fit()
        
        # 5. Print Summary
        print(ols_model.summary())
        
    except Exception as e:
        print(f"Error running regression for {outcome_var}: {e}")
```
Found very little significance, with p-values all being high, and a large amount of multicollinearity. Therefore, I started to look for potential interaction terms. The assumption for these results is due to socioeconomic variables having too much correlation. 
```{python}
#| eval: false 
#| code-summary: "P value"
# Create Interaction Term
# We use (100 - Pct_Degree) as a proxy for "Low SES" to avoid VIF issues with Income
final_df['Pct_No_Degree'] = 100 - final_df['Pct_Degree']
final_df['Noise_Interaction'] = final_df['avg_noise_db'] * final_df['Pct_No_Degree']

features_interaction = [
    'avg_noise_db',
    'Noise_Interaction', 
    'Pct_No_Degree',
    'Log_Income',
    'Total_Pop',
    'Pct_Asian', 
    'Pct_Black', 
    'Pct_Hispanic', 
    'Pct_Senior'
]

# Run just for Obesity and Depression (your strongest signals)
for outcome in ['Depression among adults']:
    if outcome in final_df.columns:
        df_mod = final_df.dropna(subset=features_interaction + [outcome])
        X = sm.add_constant(df_mod[features_interaction])
        y = df_mod[outcome]
        
        model = sm.OLS(y, X).fit()
        
        print(f"\nOUTCOME: {outcome}")
        print(f"Interaction P-Value: {model.pvalues['Noise_Interaction']:.4f}")
        print(f"R-Squared: {model.rsquared:.3f}")
        
        if model.pvalues['Noise_Interaction'] < 0.1:
            print(">>> SIGNIFICANT INTERACTION FOUND! Noise hurts populations with lower education more.")
        else:
            print("Interaction not significant.")
```
![interaction model](images/Interaction%20model.png)

#### Inital Assumption:
Noise pollution is an equity issue. It disproportionately impacts mental health in lower-income communities, while wealthier residents are buffered from its effects.

Wealthy/High-Degree Areas: The noise doesn't bother them as much. Maybe they have better soundproofing, better windows, or the noise is "positive" noise (bustling cafes) rather than "negative" noise (highways/industrial).

Lower-Degree Areas: The relationship between Noise and Depression is stronger. Here, noise acts as a stressor that compounds with financial stress.

### Looking for other interactions using this model
Created a new predictor that is 'avg_noise_db' * 'Pct_Degree' to represent more than just the people with no degree like before. 

```{python}
#| eval: false 
#| code-summary: "Other interactions"
final_df['Noise_x_Education'] = final_df['avg_noise_db'] * final_df['Pct_Degree']
predictors = [
    'avg_noise_db',
    'Noise_x_Education', 
    'Pct_No_Degree',
    'Log_Income',
    'Total_Pop',
    'Pct_Asian', 
    'Pct_Black', 
    'Pct_Hispanic', 
    'Pct_Senior'
]

for outcome_var in outcomes_of_interest:
    
    # Safety check: Ensure column exists
    if outcome_var not in final_df.columns:
        continue

    # Prepare Data
    regression_df = final_df.dropna(subset=predictors + [outcome_var]).copy()
    
    X = regression_df[predictors]
    y = regression_df[outcome_var]
    X = sm.add_constant(X)

    # Fit Model
    try:
        model = sm.OLS(y, X).fit()
        
        # Extract P-value for the interaction term
        interaction_p_val = model.pvalues.get('Noise_x_Education', 1.0)
        interaction_coeff = model.params.get('Noise_x_Education', 0.0)
        
        print(f"\n{'='*60}")
        print(f"OUTCOME: {outcome_var}")
        print(f"{'='*60}")
        
        # Check significance (P < 0.05)
        if interaction_p_val < 0.05:
            print(f"*** SIGNIFICANT INTERACTION DETECTED! ***")
            print(f"Interaction P-Value: {interaction_p_val:.4f}")
            print(f"Interaction Coeff:   {interaction_coeff:.4f}")
            if interaction_coeff < 0:
                print("INTERPRETATION: Negative coefficient. Higher education REDUCES the harmful effect of noise.")
            else:
                print("INTERPRETATION: Positive coefficient. Higher education INCREASES the effect (unlikely) or Noise hurts high-edu more.")
        else:
            print(f"No significant interaction (P = {interaction_p_val:.3f}).")
            
        print("-" * 30)
        # Uncomment the line below if you want to see the full table for every single one
        # print(model.summary())

    except Exception as e:
        print(f"Error processing {outcome_var}: {e}")
```

![High Cholesterol Interaction](images/High%20cholestrol%20interaction.png)
![Depression Interactions](images/depression_interaction.png)

Depression along with High Cholesterol appeared to have significant interactions. 

### Plotting
```{python}
#| eval: false 
#| code-summary: "Plotting"

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# INTERACTION PLOT: Noise vs Depression by Education Level
# 1. Create a Categorical Education Variable for plotting
# (Split the continuous 'Pct_Degree' into 'Low' and 'High' groups based on the median)
median_edu = final_df['Pct_Degree'].median()
final_df['Education_Level'] = final_df['Pct_Degree'].apply(lambda x: 'High Education' if x > median_edu else 'Low Education')

# 2. Set up the plot
plt.figure(figsize=(10, 6))

# 3. Plot the Regression Lines (lmplot)
# This draws two lines: one for High Edu, one for Low Edu
sns.lmplot(
    data=final_df, 
    x='avg_noise_db', 
    y='Depression among adults', 
    hue='Education_Level',   # This creates the split
    palette={'High Education': 'blue', 'Low Education': 'red'},
    height=6, 
    aspect=1.5,
    ci=95  # Show confidence intervals
)

plt.title('Interaction Effect: Noise Impact on Depression by Education Level')
plt.xlabel('Average Noise (dB)')
plt.ylabel('Depression Prevalence (%)')
plt.grid(True, alpha=0.3)
plt.show()
```
![Interaction between noise and depression](images/noise_depression.png)
#### Impact of noise on depression accounting for education level:
* Relative Deprivation: Higher socioeconomic standard individuals may feel more "entitled" to quiet environments. When that expectation is violated, the psychological toll (annoyance, frustration, depression) is higher.

* Awareness vs. Survival: Lower socioeconomic standard populations often face a "hierarchy of needs" where immediate stressors (financial instability, food security, crime) outweigh environmental noise. They may "tune out" the noise psychologically because it is just one of many stressors, resulting in lower reported noise-related depression, even if the noise is high.

![Interaction between noise and cholesterol](images/noise_choles.png)
#### Impact of noise on cholesterol accounting for education level:
* The Body Keeps the Score: Even if lower-education populations do not report higher depression, their bodies are biologically reacting to the noise stressor. The noise is triggering cortisol production and raising cholesterol, regardless of their conscious "acceptance" of the noise.

* Protective Factors for High SES: The highly educated group might be annoyed by the noise, but they have the resources to mitigate the physical damage (better diet, healthcare access, better sound-insulation in homes), keeping their cholesterol levels decoupled from the noise levels.

### Cluster Analysis
Conducted cluster analysis to see the spatial split of how these factors impact specific communities. 
```{python}
#| eval: false 
#| code-summary: "P value"

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Select the features for clustering
# We want to cluster on Environmental + Demographic factors
cluster_features = [
    'Log_Noise_Exposure',
    'Noise_Interaction', 
    'Pct_No_Degree',
    'Log_Income',
    'Total_Pop',
    'Pct_Asian', 
    'Pct_Black', 
    'Pct_Hispanic', 
    'Pct_Senior'
]

# Drop NaNs just for the clustering subset
df_cluster = final_df.dropna(subset=cluster_features).copy()

# 2. Scale the Data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_cluster[cluster_features])

# Calculate "Inertia" for K=1 to K=10
inertia = []
K_range = range(1, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot the Elbow
plt.figure(figsize=(8, 4))
plt.plot(K_range, inertia, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Sum of Squared Distances)')
plt.title('Elbow Method')
plt.grid(True)
plt.show()
```
![K cluster](images/Elbow.png)
While the drop is great between cluster 1 and 2, it greatly decreases between 2 and 3 and 4. Too vague to fully tell which one to use

```{python}
#| eval: false 
#| code-summary: "Silhouette Score"

from sklearn.metrics import silhouette_score

for k in [2, 3, 4, 5]:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    print(f"For k={k}, the Silhouette Score is: {score:.3f}")
```
![Silhouette score](images/silhouette%20score.png)
While k=2 is the highest silhouette score, having only 2 clusters isn't going to be helpful.

Therefore, I will be using k=4 as the second highest score 

```{python}
#| eval: false 
#| code-summary: "K=4"

k = 4  

# 1. Fit the Model
kmeans = KMeans(n_clusters=k, random_state=42)
df_cluster['Cluster_Labels'] = kmeans.fit_predict(X_scaled)

# 2. Merge labels back to main dataframe (if indices match)
final_df['Cluster'] = df_cluster['Cluster_Labels']

# 3. INTERPRET: What does each cluster represent?
# We group by Cluster and calculate the mean of each feature
summary = df_cluster.groupby('Cluster_Labels')[cluster_features].mean()

# Add a count to see how many tracts are in each cluster
summary['Count'] = df_cluster['Cluster_Labels'].value_counts()

# Add 'Noise_dBA' for easier reading if you created it earlier
if 'Log_Noise_Exposure' in summary.columns:
    summary['Noise_dBA'] = np.exp(summary['Log_Noise_Exposure'])

print("--- CLUSTER PROFILES (Mean Values) ---")
print(summary.round(2).T) 
```
![cluster](images/4cluster.png)
Cluster 0: Highest noise, lowest income/education. oldest pop, and white
(low income, old, white pop with high noise)

Cluster 1: average across the board
(middle ground)

Cluster 2: Highest income, high education, relatively high noise, very diverse, and pop density
(major urban center with wealthy educated and diverse people, still high noise)

Cluster 3: Lowest noice, highest education
(Wealthy highly educated suburbs with little noise stress)

#### Boxplot
```{python}
#| eval: false 
#| code-summary: "cluster box"

# Boxplot of Depression by Cluster
plt.figure(figsize=(10, 6))
sns.boxplot(x='Cluster', y='Depression among adults', data=final_df, palette='Set2')
plt.title('Depression Prevalence by Neighborhood Cluster')
plt.ylabel('Depression %')
plt.xlabel('Cluster Label')
plt.show()

# Boxplot of cholestrol by Cluster
plt.figure(figsize=(10, 6))
sns.boxplot(x='Cluster', y='High cholesterol among adults who have ever been screened', data=final_df, palette='Set2')
plt.title('Cholesterol Prevalence by Neighborhood Cluster')
plt.ylabel('Cholesterol Prevalence')
plt.xlabel('Cluster Label')
plt.show()
```

![cluster Graph](images/cluster_graph.png)
![cluster Cholesterol](images/cluster_cholesterol.png.png)

Cluster 2: low depression and high cholestrol. (wealthy, diverse, and living in high density and high noise environments. They don't report feeling depressed but they have the highest cholesterol showing worst physiological stress indicators.)

Cluster 3: low cholestrol but high depression (has the quiet but bad mental health. Could be that they are accustomed to quiet environments, that they have a lower threshold for noise causing them distress.)

Cluster 0: low income, older, and high noise exposer. They get hit with everythihng 

```{python}
#| eval: false 
#| code-summary: "map cluster"

import contextily as ctx

# Ensure final_df is a GeoDataFrame
if hasattr(final_df, 'plot'):
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # Plot clusters
    final_df.plot(column='Cluster', 
                  categorical=True, 
                  legend=True, 
                  cmap='tab10', 
                  ax=ax,
                  alpha=0.6,
                  edgecolor='white',
                  linewidth=0.5)
    try:
        ctx.add_basemap(ax, crs=final_df.crs.to_string())
    except:
        pass

    ax.set_axis_off()
    ax.set_title(f'Neighborhood Typologies (K-Means Clustering, k={k})')
    plt.show()
else:
    print("final_df is not a GeoDataFrame. Skipping map.")
```

![map](images/cluster_map.png)

Based on the box plots and spatial analysis, these findings reveal a striking "Paradox of Place" where psychological well-being and physiological health are inversely related across neighborhood types.

Cluster 2 (The Cosmopolitan Core) presents the most compelling contradiction: residents here report the lowest rates of depression (~15%) yet suffer from the highest rates of high cholesterol (~36%). This suggests a "Cosmopolitan Buffer," where the economic opportunities, social vibrancy, and amenities of dense urban living protect mental health, causing residents to psychologically "discount" the environmental stressors. However, their bodies still keep the score—the high noise and density create a hidden physiological tax (allostatic load) that manifests as high cholesterol, even if they don't feel "stressed."

Conversely, Cluster 3 (The Quiet Elite) shows the exact opposite pattern: they have the lowest cholesterol (indicating good physical health and protective resources) but the highest reported depression (~24%). This supports the "Expectation of Quiet" hypothesis; residents in these privileged, lower-noise areas may be more sensitive to environmental disturbances or suffer from suburban isolation, leading to higher reported mental distress despite their physical safety. Meanwhile, Cluster 0 (The Vulnerable Periphery) lacks the protective buffers of either group, showing moderately high levels of both depression and cholesterol, reflecting the compounding burdens of lower socioeconomic status and environmental exposure.