{
  "hash": "aca7d2b24720ce667f533030a154aa26",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Analysis Process\"\nexecute:\n    warning: false\n    message: false\n---\n\n## 1. Environment Setup & Configuration\n\nWe start by importing the necessary libraries for geospatial analysis and machine learning.\n\n::: {#e2e6a983 .cell execution_count=1}\n``` {.python .cell-code code-summary=\"Importing Libraries\"}\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport rasterio\nimport subprocess\nimport fiona\nfrom rasterstats import zonal_stats\nfrom sodapy import Socrata\nfrom shapely.geometry import shape\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport warnings\n\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")\n```\n:::\n\n\n## 2. Data Gathering\n\n### Processing Noise Raster Data\n\nThe National Transportation Noise Map is provided as individual high-resolution TIFF files for each state. To analyze the Northeast as a contiguous region, I needed to merge these into a single mosaic.\n\nBecause the uncompressed raster data exceeds 50GB, merging them into a physical file is inefficient and computationally expensive. Instead, I utilized GDAL to build a Virtual Raster (VRT). A VRT acts as a pointer file that treats the collection of state rasters as a single seamless map without duplicating the underlying data.\n\n::: {#noise-processing .cell execution_count=2}\n``` {.python .cell-code code-summary=\"VRT Construction Code\"}\nimport os\ndata_dir = \"data/CONUS_rail_road_and_aviation_noise_2020/State_rasters/\"\nsearch_pattern = os.path.join(data_dir, \"*_rail_road_and_aviation_noise_2020.tif\")\noutput_vrt = os.path.join(data_dir, \"CONUS_merged_noise_map_2020.vrt\")\n\ntif_files = glob.glob(search_pattern)\n\nif not tif_files:\n    print(f\"--- ERROR ---\")\n    print(f\"No .tif files were found at: {search_pattern}\")\nelse:\n    print(f\"Found {len(tif_files)} state raster files to link.\")\n    \n    command_list = [\"gdalbuildvrt\", output_vrt] + tif_files\n    \n    print(\"Building VRT file...\")\n    \n    try:\n        subprocess.run(command_list, check=True, shell=True)\n        \n        print(f\"\\n--- Success! ---\")\n        print(f\"Virtual Raster .vrt file created at:\")\n        print(output_vrt)\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"\\n--- GDAL ERROR ---\")\n        print(f\"The command failed: {e}\")\n        print(\"Please ensure GDAL is installed in your active Conda environment.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n```\n:::\n\n\n### Data Ingestion Pipeline\n\nTo streamline the process, I created a **DataPipeline** class. This class handles the connection to the CDC API, downloads county boundaries, and manages the noise raster file paths.\n\n::: {#pipeline-class .cell execution_count=3}\n``` {.python .cell-code}\nclass DataPipeline:\n    def __init__(self, cdc_token, noise_path):\n        self.cdc_token = cdc_token\n        self.noise_path = noise_path\n        self.NE_STATES = ['PA', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'VT', 'ME']\n\n    def get_cdc_data(self, limit=50000):\n        client = Socrata(\"data.cdc.gov\", self.cdc_token)\n        dataset_id = \"swc5-untb\"\n        results = client.get(dataset_id, limit=limit)\n        \n        gdf = gpd.GeoDataFrame.from_records(results)\n        \n        # Geometry processing\n        gdf = gdf.dropna(subset=['geolocation'])\n        gdf['geometry'] = gdf['geolocation'].apply(lambda x: shape(x))\n        gdf = gdf.set_geometry('geometry', crs=\"EPSG:4326\")\n        \n        # Filter for NE States\n        gdf = gdf[gdf['stateabbr'].isin(self.NE_STATES)]\n        \n        # Numeric conversion\n        gdf['data_value'] = pd.to_numeric(gdf['data_value'], errors='coerce')\n        return gdf\n\n    def get_counties(self):\n        url = \"https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\"\n        counties = gpd.read_file(url)\n        counties = counties.to_crs(\"EPSG:4326\")\n        \n        # Create FIPS\n        counties['FIPS'] = counties['STATEFP'] + counties['COUNTYFP']\n        return counties[['FIPS', 'geometry']]\n```\n:::\n\n\n### Loading the Data\n\nNote: Since API calls and spatial joins are computationally expensive, the code below demonstrates the logic used. For this website report, we load the pre-processed GeoJSON files directly.\n\n::: {#a452e454 .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Data Loading Logic\"}\n# Initialize Pipeline\n# CDC_APP_TOKEN = \"YOUR_TOKEN\"\n# pipeline = DataPipeline(CDC_APP_TOKEN, \"path/to/noise.vrt\")\n# cdc_gdf = pipeline.get_cdc_data(limit=250000)\n\n# Pivot CDC Data (Wide Format)\n# We need rows = Counties, Columns = Health Outcomes + SES\n# health_pivot = cdc_gdf.pivot_table(\n#    index=['locationid', 'locationname', 'stateabbr'],\n#    columns='measure',\n#    values='data_value',\n#    aggfunc='mean'\n# ).reset_index()\n\n# model_gdf = counties_gdf.merge(health_pivot, left_on='FIPS', right_on='locationid', how='inner')\n```\n:::\n\n\n::: {#89089176 .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Load Pre-processed Data\"}\n# Loading locally saved data for report generation\nimport geopandas as gpd\n# Ensure this path matches your project structure\ntry:\n    model_gdf = gpd.read_file(\"data/counties_gdf.geojson\")\nexcept:\n    print(\"Local data file not found. Please ensure 'data/counties_gdf.geojson' exists.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLocal data file not found. Please ensure 'data/counties_gdf.geojson' exists.\n```\n:::\n:::\n\n\n### Fetching Census Socioeconomic Data\n\nWe target the **2023 American Community Survey (ACS) 5-Year Estimates** for this analysis. The 5-year estimates are preferred over 1-year data for this project because they provide higher statistical precision for areas with smaller populations, smoothing out annual volatility in rural counties. We specifically query variables that serve as robust proxies for Social Determinants of Health (SDOH):\n\n* **Economic Stability:** `Median_Income` and `Poverty_Count` are variables which measure community wealth and economic deprivation. Low-income communities often face higher exposure to environmental stressors and have fewer resources to mitigate them (e.g., soundproofing, healthcare access).\n* **Environmental Justice:** `White_Count` and `Black_Count` are included to test for potential disparities in noise exposure. Environmental justice literature suggests that transportation infrastructure is disproportionately sited near communities of color.\n* **Education:** `Bachelor_Degree_Count` is included as educational attainment is a strong predictor of health literacy and overall socioeconomic status, often providing explanatory power beyond income alone.\n\n::: {#01d7c388 .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Census API Logic\"}\ndef fetch_census_data_standalone():\n    \n    # NE State FIPS: CT, ME, MA, NH, NJ, NY, PA, RI, VT\n    ne_fips = [\"09\", \"23\", \"25\", \"33\", \"34\", \"36\", \"42\", \"44\", \"50\"]\n    # Define variables\n    vars_map = {\n        \"B01003_001E\": \"Total_Pop\",\n        \"B19013_001E\": \"Median_Income\",\n        \"B17001_002E\": \"Poverty_Count\",\n        \"B03002_003E\": \"White_Count\",\n        \"B03002_004E\": \"Black_Count\",\n        \"B15003_022E\": \"Bachelor_Degree_Count\"\n    }\n\n# API Fetching Logic Hidden for Brevity\n# See GitHub repo for full implementation\n```\n:::\n\n\n::: {#0908f1e1 .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Load Census Data\"}\nimport pandas as pd\nimport numpy as np\n# Load pre-fetched census data\n# Using relative path for portability\ntry:\n    census_df = pd.read_csv(\"data/census_df.csv\", dtype={'FIPS': str})\n    \n    # Quick feature engineering if not already done in CSV\n    if 'Log_Income' not in census_df.columns:\n         census_df['Log_Income'] = np.log1p(census_df['Median_Income'])\nexcept:\n    print(\"Local census file not found.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLocal census file not found.\n```\n:::\n:::\n\n\n### Standardization & Merge\n\nBefore merging, we must ensure our primary keys match perfectly. FIPS (Federal Information Processing Standards) codes are often read as integers by default, which strips the leading zero from states like Alabama, California, or Connecticut.\n\n::: {#ccb0bccc .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Standardization\"}\n# Ensure 5-digit FIPS\nmodel_gdf['FIPS'] = model_gdf['locationid'].astype(str).str.zfill(5)\ncensus_df['FIPS'] = census_df['FIPS'].astype(str).str.zfill(5)\n\n# Left Join to retain spatial geometry\nfinal_df = model_gdf.merge(census_df, left_on='FIPS', right_on='FIPS', how='left')\n\n# Feature Engineering: Log Noise\n# Y = ln(1 + Noise_db)\nfinal_df['Log_Noise_Exposure'] = np.log1p(final_df['avg_noise_db'])\n```\n:::\n\n\n### Utility Functions\n\nWe also defined helper functions for multicollinearity checks (VIF) and correlation plotting.\n\n::: {#utility-function .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Helper Functions\"}\ndef calculate_vif(df):\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n    return vif_data.sort_values('VIF', ascending=False)\n\ndef plot_correlation_matrix(df, title=\"Correlation Matrix\"):\n    plt.figure(figsize=(10, 8))\n    corr = df.corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    sns.heatmap(corr, mask=mask, cmap='RdBu_r', center=0, square=True, linewidths=.5)\n    plt.title(title, fontsize=14)\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n",
    "supporting": [
      "data-analysis_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}